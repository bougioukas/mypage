{
  "hash": "48342deda006247d4dbdf62edee66fff",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Perceptron: the first artificial neuron\"\ndescription: \"The perceptron is one of the earliest and simplest models of an artificial neuron, introduced by Frank Rosenblatt in 1957 as a computational approach inspired by biological neurons.\"\nauthor: \n    - name: Konstantinos Bougioukas \ndate: \"2026-01-03\"\ncategories: [Statistics]\nimage: \"perceptron.png\"\ncss: /custom.css\n#draft: true\n---\n\n\n\n​\n\n## The perceptron\n\nThe perceptron is one of the earliest and simplest models of an artificial neural network. Conceptually, it corresponds to a single artificial neuron that makes binary decisions. Despite its simplicity, the perceptron introduced the core ideas of weighted inputs, linear decision boundaries, and supervised learning for classification tasks—ideas that remain central to modern deep learning.\n\nAt its heart, the perceptron learns to separate data into two classes by finding a linear boundary that divides the input space. Each prediction is obtained by combining the inputs linearly and passing the result through a threshold function that produces a discrete output.\n\n\n\n### How it works\n\nThe *Perceptron* can be viewed as a simple directed graph, with edges connecting each input to the output and parameterized by weights. The output of the perceptron depends both on the values of the input nodes and on the weights associated with the incoming edges, which control how strongly each input influences the final decision.\n\n\n![A simple Perceptron model with weights and activation.](perceptron.png){width=50%}\n\n​\n\n\n**Basic set up and notation**\n\nThe *Perceptron* takes multiple input features, multiplies each by an associated weight, sums these weighted inputs along with a bias term, and then applies a threshold decision (activation) function to determine the output class. This process can be described compactly using vector notation.\n\nSuppose our dataset consists of $P$ input-output pairs \n\n$$(\\mathbf{x_1}, y_1), (\\mathbf{x_2}, y_2), ..., (\\mathbf{x_P}, y_P)$$ \n\nor equivalently, \n\n\n$$\\left\\{ \\left(\\mathbf{x}_{p},y_{p}\\right)\\right\\} _{p=1}^{P}$$\n\nwhere $\\mathbf{x_p}$ and $y_p$ denote the input and output of the $p-$observation, respectively.\n\nEach input $\\mathbf{x_p}$ is a column vector of length $N +1$, where the leading component is fixed to 1 in order to incorporate the bias term directly into the model: \n\n$$\\mathbf{x_p} = \\begin{bmatrix} 1 \\\\ x_{1,\\ p} \\\\ x_{2,\\ p} \\\\\\vdots \\\\ x_{N,\\ P} \\end{bmatrix}$$\n\nThe model parameters—the bias and feature weights—are collected into a single column vector:\n\n$$\\mathbf{w} = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_N \\end{bmatrix}$$\n\n\nThe target labels are represented by a vector $\\mathbf{y}$ of length $P$ : \n\n$$\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_P \\end{bmatrix}$$\n\n\n**Linear Combination**\n\nFor a single observation $p$, the perceptron computes a net input $z_p$ by taking the dot product of the weight vector and the input vector:\n\n\n$$z_p = \\mathbf{w}^{\\mathsf{T}}\\mathbf{x}_p$$\n\nwhere the superscript $\\mathsf{T}$ denotes transposition.\n\n\nExpanding this expression makes the computation explicit:\n\n$$\\begin{aligned}\nz_p &= \\begin{bmatrix} w_0 & w_1 & \\cdots & w_N \\end{bmatrix} \\begin{bmatrix} 1 \\\\ x_{1,\\ p} \\\\ x_{2,\\ p} \\\\\\vdots \\\\ x_{N,\\ P} \\end{bmatrix} \\\\\n&= (w_0 \\cdot 1) + (w_1 \\cdot x_{1,p}) + \\dots + (w_N \\cdot x_{N,P})  \\\\\n&= \\underbrace{\\enspace w_0 \\enspace}_{\\text{bias}} + \\underbrace{\\sum_{n=1}^{N} w_{n} \\cdot x_{n, p}}_{\\text{sum of weights}}\n\\end{aligned}$$\n\n\nThus, the net input is simply a **weighted sum** of the input features plus a bias term.\n\n​\n\nComputing this quantity for all P observations yields the vector $\\mathbf{z}$ with p-elements: \n\n$$\\mathbf{z}= \\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_P \\end{bmatrix}$$\n\n​\n\n\n**Activation function and model prediction**\n\nIn the classic perceptron, the **activation function** $f(\\cdot)$ is a step function, which maps the continuous net input to one of two discrete class labels. A common choice is the **sign function**, which assigns a label based on whether the net input is non-negative or negative.\n\n\nThe prediction vector $\\mathbf{\\hat{y}}$ is obtained by applying the activation function element-wise to $\\mathbf{z}$:\n\n$$\\mathbf{\\hat{y}} = \\text{sign}(\\mathbf{z})$$\n\nIn expanded form, this becomes\n\n$$\\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_P \\end{bmatrix} = \\begin{bmatrix} \\text{sign}(z_1) \\\\ \\text{sign}(z_2) \\\\ \\vdots \\\\ \\text{sign}(z_P) \\end{bmatrix}$$\n\n\nEach individual prediction $\\hat y_p$ is defined as\n\n$$\\hat y_p = \\text{sign}(z_p) = \\begin{cases} 1 & \\text{if } z_p \\ge 0 \\\\ -1 & \\text{if } z_p < 0 \\end{cases}$$\n\nThrough this simple rule, the perceptron partitions the input space into two regions separated by a linear decision boundary, classifying inputs on one side as $+1$ and those on the other side as $-1$, which is why it is considered a linear binary classifier; this decision boundary is defined as the set of points of the input space where the model’s net input is exactly zero, that is, $\\mathbf{x}_p^{\\mathsf{T}}\\mathbf{w} = 0$.\n\n​\n\n**The Weight Update Rule**\n\nThe perceptron learns by iteratively adjusting its weights to reduce classification errors. Let $\\mathbf{w}^{\\text{old}}$ be the weight vector before the update, and $\\mathbf{w}^{\\text{new}}$ be the weight vector after the update. The learning rule is expressed as:\n\n$$\\mathbf{w}^{\\text{new}} = \\mathbf{w}^{\\text{old}} + \\eta \\, (y_p - \\hat{y}_p) \\, \\mathbf{x}_p$$\n\nwhere $\\eta > 0$ is the learning rate, controlling the step size of each update. The error term $(y_p - \\hat{y}_p)$ is a discrete scalar that determines the nature of the weight update.\n\n\nThe perceptron learning rule adjusts the weight vector in the direction that reduces the error for the specific example $p$:\n\n- **Correct prediction** ($y_p = \\hat{y}_p$): Since $(y_p - \\hat{y}_p) = 0$, the update term is zero. No change is made, as the model’s current boundary already correctly classifies the point.\n\n- **Incorrect prediction** ($y_p \\neq \\hat{y}_p$): The term $(y_p - \\hat{y}_p)$ becomes either $+2$ or $-2$. The weights are \"nudged\" in the direction of $y_p \\mathbf{x}_p$, moving the decision boundary.\n\n​\n\n**Convergence and separability**\n\nTraining proceeds in **epochs**, where each epoch corresponds to a complete pass through the training dataset. Because the perceptron updates its weights incrementally after each individual example, learning typically requires multiple epochs as the decision boundary is gradually adjusted to better separate the data. Whether this process eventually halts—reaching a state known as **convergence**—depends entirely on the linear separability of the training set.\n\nWhen the classes are **linearly separable**, meaning they can be perfectly divided by a straight line in two dimensions or a hyperplane in higher dimensions, the Perceptron Convergence Theorem guarantees that the algorithm will reach a finite weight vector that correctly classifies all training examples. At this point, the weights stabilize and no further updates occur. Conversely, if the data are not linearly separable, no such solution exists. The perceptron will continue updating its weights indefinitely, repeatedly shifting the decision boundary in an attempt to correct misclassifications. Fixing one misclassified point may cause another to become misclassified, producing oscillatory or “jittering” behavior. In practice, training on **non-separable** data is terminated after a fixed number of epochs to prevent unbounded computation.\n\n\n\n\n\n\nThis is achieved by repeatedly permuting the data labels without replacement—so that each permutation is a rearrangement of the original labels—and recalculating the test statistic for each permutation. The resulting p-value is the proportion of permutations that produce a statistic as extreme as—or more extreme than—the observed value.\n\nWe now describe this framework in detail, step by step.\n\nLet $\\mathbf{X_1} = \\{X_{1i}, i = 1, 2, ..., n_1  \\}$ and $\\mathbf{X_2} = \\{X_{2i}, i = 1, 2, ..., n_2  \\}$ be two sets of independent sample data. The objective is to test whether the data are consistent with the null hypothesis of no difference in locations of the distributions ($\\delta = 0$, where $\\delta$ is known as the treatment effect)\n\n$$H_o: \\mathbf{X_1} \\stackrel{d}{=} \\mathbf{X_2}$$\n\nor with the two-sided alternative ($\\delta \\neq 0$)\n\n$$H_1: \\mathbf{X_1} \\stackrel{d}{\\neq} \\mathbf{X_2}$$\n\n**Step 1: Combine the two datasets**\n\nFirst, we combine the two datasets in one vector as follows:\n\n$$\\mathbf{X} = \\mathbf{X}_1 \\uplus \\mathbf{X}_2 = \\{X_i, i = 1, \\ldots, n\\}, \\ \\ \\ n= n_1 + n_2$$\n\nwhere $\\uplus$ denotes concatenation, so that the two samples are pooled into a single set $\\mathbf{X}$, which is fixed.\n\n​\n\n**Step 2: Create an assignment indicator vector**\n\nThe assignment vector $\\mathbf{Z}$ indicates which group each participant belongs to:\n\n$$\\mathbf{Z} = \\{{Z_i, i = 1, \\ldots, n}\\}$$\n\nwhere:\n\n$$\nZ_i = \n\\begin{cases}\n1, & \\text{if participant } i \\text{ is assigned to } \\mathbf{X}_1,\\\\\n0, & \\text{if participant } i \\text{ is assigned to } \\mathbf{X}_2.\n\\end{cases}\n$$\n\nProperties of $\\mathbf{Z}$:\n\n-   Each $Z_i$ is binary: $Z_i \\in \\{0, 1\\}$\n\n-   The sum $\\sum_{i=1}^{n} Z_i = n_1$ (counts how many are in Group 1)\n\n-   The sum $\\sum_{i=1}^{n} (1 - Z_i) = n_2$ (counts how many are in Group 2)\n\n​\n\nTherefore, the observed assignment indicator is:\n\n$$\n\\mathbf{Z^{\\text{obs}}} = (\\underbrace{1, 1, \\ldots, 1}_{n_1 \\text{ times}}, \\underbrace{0, 0, \\ldots, 0}_{n_2 \\text{ times}})\n$$\n\n​\n\n**Step 3: Define the test statistic (e.g., difference in means)**\n\nThe test statistic is a function of both $\\mathbf{Z}$ and $\\mathbf{X}$:\n\n$$T (\\mathbf{Z}, \\mathbf{X}) = \\mathbf{\\bar{X}_1} - \\mathbf{\\bar{X}_2}$$\n\nwhere the group means are\n\n$$\\mathbf{\\bar{X}_1} = \\frac{1}{n_1} \\sum_{i=1}^{n} Z_i X_i = \\frac{1}{n_1} \\sum_{i: Z_i = 1} X_i$$\n\n$$\\mathbf{\\bar{X}_2} = \\frac{1}{n_2} \\sum_{i=1}^{n} (1 - Z_i) X_i = \\frac{1}{n_2} \\sum_{i: Z_i = 0} X_i$$\n\nTherefore, the observed test statistic is:\n\n$$T^{obs}=T(\\mathbf{Z^{obs}},\\mathbf{X})$$\n\n​\n\n**Step 4: Generate the permutation distribution**\n\nUnder the null hypothesis $H_o$, any permutation of $\\mathbf{Z}$ is just as likely to have produced the observed data $\\mathbf{X}$ as the actual assignment $\\mathbf{Z^{obs}}$. Relying on this assumption, we randomly permute (suffle) the group labels while keeping the data values fixed. For each permutation, we recalculate the test statistic $T$.\n\nIf we consider all possible orderings of n observations:\n\n$$n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1$$\n\nthen, for example, if $n=16$, the total number of orderings (i.e., permutations of all 16 items) is enormous:\n\n$$16! \\approx 2.09 \\times 10^{13}$$\n\nHowever, for a test statistic based on group means, we are only interested in which observations belong to which group—not the order of observations within each group. The mean of a set of numbers remains the same no matter the order in which the numbers are added.\n\nIn this case, we only need to determine which observations go into the first group; the remaining observations automatically belong to the second group. The number of such distinct assignments (cardinality) is :\n\n$$M = \\ ^nC_{n_1} = \\binom{n}{n_1} = \\frac{n!}{n_1! \\, (n - n_1)!}$$ This represents choosing $n_1$ observations out of $n$ to be in Group 1. The remaining $n_2 = n - n_1$ observations then automatically form Group 2.\n\n::: callout-note\n# A simple example\n\nLet's consider a total of $n=4$ observations that are to be split into two groups: Group A with $n_1=2$ observations and Group B with $n_2=2$ observations. The pooled data set, $\\mathbf{X}$, is fixed as the set of values $\\{a_1, a_2, b_1, b_2\\}$.\n\nAssuming the specific numerical observations are $\\{1, 2, 3, 4\\}$, the possible permutations for splitting this data set into two groups of size two are:\n\n-   $A = \\{1, 2\\} | B = \\{3, 4\\}$\n-   $A = \\{1, 3\\} | B = \\{2, 4\\}$\n-   $A = \\{1, 4\\} | B = \\{2, 3\\}$\n-   $A = \\{2, 3\\} | B = \\{1, 4\\}$\n-   $A = \\{2, 4\\} | B = \\{1, 3\\}$\n-   $A = \\{3, 4\\} | B = \\{1, 2\\}$\n\n$$M = \\ ^4C_2 = \\binom{4}{2} = \\frac{4!}{2!(4-2)!} = \\frac{4 \\times 3 \\times 2 \\times 1}{(2 \\times 1)(2 \\times 1)} = \\frac{24}{4} = 6$$\n:::\n\n​\n\nThe set of all possible assignment vectors:\n\n$$\\Omega = \\Big\\{ \\mathbf{Z}^{(m)} : \\sum_{i=1}^{n} Z_i^{(m)} = n_1, \\ Z_i^{(m)} \\in \\{0,1\\}, \\ m = 1, \\dots, M \\Big\\}$$ For each $\\mathbf{Z}^{(m)} \\in \\Omega$, we compute:\n\n$$T^{(m)} = T(\\mathbf{Z}^{(m)}, \\mathbf{X})$$\n\nThis gives us the exact permutation distribution:\n\n$$\\mathcal{T} = \\{ T^{(1)}, T^{(2)}, \\dots, T^{(M)} \\}$$\n\n​\n\n**Step 5: Calculate the p-value**\n\nFor a two-sided exact p-value:\n\n$$\np\n=\n\\begin{cases}\n\\displaystyle\n\\frac{1}{M}\n\\sum_{m=1}^{M}\n\\Big[\n\\mathbf{I}\\!\\left(T^{(m)} \\ge T^{\\text{obs}}\\right)\n+\n\\mathbf{I}\\!\\left(T^{(m)} \\le -T^{\\text{obs}}\\right)\n\\Big],\n& if \\ \\ T^{\\text{obs}} > 0,\n\\\\[1.2em]\n\\displaystyle\n\\frac{1}{M}\n\\sum_{m=1}^{M}\n\\Big[\n\\mathbf{I}\\!\\left(T^{(m)} \\le T^{\\text{obs}}\\right)\n+\n\\mathbf{I}\\!\\left(T^{(m)} \\ge -T^{\\text{obs}}\\right)\n\\Big],\n& if \\ \\ T^{\\text{obs}} < 0.\n\\end{cases}\n$$\n\nor more compactly\n\n$$\np\n=\n\\frac{1}{M}\n\\sum_{m=1}^{M}\n\\mathbf{I}\\!\\left(\n\\left|T^{(m)}\\right|\n\\ge\n\\left|T^{\\text{obs}}\\right|\n\\right)\n$$\n\nwhere the indicator function $\\mathbf{I}(\\text{condition}) =\n\\begin{cases}\n1, & \\text{if condition is true},\\\\\n0, & \\text{otherwise}.\n\\end{cases}$\n\n### Monte Carlo simulation\n\nWhen $M$ is large (computationally unfeasible to enumerate all permutations), we use Monte Carlo simulation. In this case, we generate random permutation $Z^{(b)}$ and compute the test statistic on the permuted labels $T^{(b)} = T(\\mathbf{Z}^{(b)}, \\mathbf{X}), \\ where \\ b = 1, \\dots, B$ and $B$ is the number of Monte Carlo permutations. This creates the approximate permutation distribution $\\mathcal{T_{aprox}} = \\{T^{(1)}, T^{(2)}, \\ldots, T^{(B)}\\}$ and the p-value is calculated as:\n\n$$\n\\hat{p}_{\\text{MC}}\n=\n\\frac{1}{B}\n\\sum_{b=1}^{B}\n\\mathbf{I}\\!\\left(\n\\left|T^{(b)}\\right|\n\\ge\n\\left|T^{\\text{obs}}\\right|\n\\right)\n$$\n\n​\n\n## Numerical example: application of the framework in R\n\nLet the sample data\n\n$$\\mathbf{X}_1 = \\{66, 57, 81, 62, 61, 60, 73, 53\\} \\quad (n_1=8)$$\n\n$$\\mathbf{X}_2 = \\{64, 58, 59, 44, 47, 56, 48, 51\\} \\quad (n_2=8)$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data\nX1 <- c(66, 57, 81, 62, 61, 60, 73, 53)   # (n1=8)\nX2 <- c(64, 58, 59, 44, 47, 56, 48, 51)   # (n2=8)\n```\n:::\n\n\n\n​\n\n**Step 1: Combine the two datasets**\n\nFollowing the permutation test procedure, the two samples are pooled into a single vector $\\mathbf{X}$ of length $n=16$:$$\\mathbf{X} = \\mathbf{X}_1 \\uplus \\mathbf{X}_2 = \\{66, 57, 81, 62, 61, 60, 73, 53, 64, 58, 59, 44, 47, 56, 48, 51\\}$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn1 <- length(X1)\nn2 <- length(X2)\nn <- n1 + n2\nX <- c(X1, X2)\nX\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 66 57 81 62 61 60 73 53 64 58 59 44 47 56 48 51\n```\n\n\n:::\n:::\n\n\n\n​\n\n**Step 2: Create the observed assignment vector**\n\nThe observed assignment vector is:\n\n$$\n\\mathbf{Z^{\\text{obs}}} = (\\underbrace{1, 1, 1, 1, 1, 1, 1, 1}_{8 \\text{ times}}, \\underbrace{0, 0, 0, 0, 0, 0, 0, 0}_{8 \\text{ times}})\n$$\n\nTherefore:\n\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\mathbf{X}$ | 66 | 57 | 81 | 62 | 61 | 60 | 73 | 53 | 64 | 58 | 59 | 44 | 47 | 56 | 48 | 51 |\n| $\\mathbf{Z^{\\text{obs}}}$ | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\n​\n\nPermutation $m=1$ (Original Assignment)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Indices for Group 1 (X1) in the pooled vector X\nperm1 <- c(1, 2, 3, 4, 5, 6, 7, 8)\nperm1 \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 2 3 4 5 6 7 8\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Initialize the assignment vector Z to all zeros\nZ_1 <- numeric(16)\nZ_1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assign 1s to the indices corresponding to Group 1\nZ_1[perm1] <- 1    # Put 1s at positions 1-8\nZ_1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nZ_obs <- Z_1\nZ_obs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n```\n\n\n:::\n:::\n\n\n\n​\n\n**Step 3: Calculate the observed test statistic**\n\nThe $T^{obs}$ statistic is:\n\n$$T^{obs} = T(\\mathbf{Z}^{(obs)}, \\mathbf{X})$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX1_mean <- mean(X1)\nX2_mean <- mean(X2)\nT_obs <- X1_mean - X2_mean\nT_obs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10.75\n```\n\n\n:::\n:::\n\n\n\n​\n\n**Step 4: Generate the permutation distribution**\n\nNext, the following two permutations are presented (i.e., m = 2 and m = 3).\n\n-   For permutation $m=2$ (Different Assignment; the last element is changed to 9)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Indices for Group 1 (X1) in the pooled vector X\nperm2 <- c(1, 2, 3, 4, 5, 6, 7, 9)     # now last element is 9\nperm2 \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 2 3 4 5 6 7 9\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Initialize the assignment vector Z to all zeros\nZ_2 <- numeric(16)\nZ_2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assign 1s to the indices corresponding to Group 1\nZ_2[perm2] <- 1    # Put 1s at positions 1-7 and 9\nZ_2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nX1_perm2 <- X[Z_2 == 1]\nX1_perm2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 66 57 81 62 61 60 73 64\n```\n\n\n:::\n\n```{.r .cell-code}\nX2_perm2 <- X[Z_2 == 0]\nX2_perm2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53 58 59 44 47 56 48 51\n```\n\n\n:::\n:::\n\n\n\nThe T statistic is:\n\n$$T^{(2)} = T(\\mathbf{Z}^{(2)}, \\mathbf{X})$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nT_perm2 <- mean(X1_perm2) - mean(X2_perm2)\nT_perm2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 13.5\n```\n\n\n:::\n:::\n\n\n\n​\n\n-   Similarly, for permutation $m=3$ (Different Assignment; the last element is changed to 10)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Indices for Group 1 (X1) in the pooled vector X\nperm3 <- c(1, 2, 3, 4, 5, 6, 7, 10)    # now the last element is 10\nperm3 \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  1  2  3  4  5  6  7 10\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Initialize the assignment vector Z to all zeros\nZ_3 <- numeric(16)\nZ_3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assign 1s to the indices corresponding to Group 1\nZ_3[perm3] <- 1    # Put 1s at positions 1-7 and 10\nZ_3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nX1_perm3 <- X[Z_3 == 1]\nX1_perm3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 66 57 81 62 61 60 73 58\n```\n\n\n:::\n\n```{.r .cell-code}\nX2_perm3 <- X[Z_3 == 0]\nX2_perm3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53 64 59 44 47 56 48 51\n```\n\n\n:::\n:::\n\n\n\nThe T statistic is:\n\n$$T^{(3)} = T(\\mathbf{Z}^{(3)}, \\mathbf{X})$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nT_perm3 <- mean(X1_perm3) - mean(X2_perm3)\nT_perm3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12\n```\n\n\n:::\n:::\n\n\n\n​\n\nNow, the total number of possible permutations is computed as:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM <- choose(n, n1)\nM\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12870\n```\n\n\n:::\n:::\n\n\n\nIn this case, it is feasible to generate all possible permutations:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate  all possible combinations of choosing n1 elements from the set X\nlibrary(combinat)\nall_perms <- combn(1:n, n1)\n```\n:::\n\n\n\nThe first three permutations, as found previously, are:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_perms[, c(1:3)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    2    2    2\n[3,]    3    3    3\n[4,]    4    4    4\n[5,]    5    5    5\n[6,]    6    6    6\n[7,]    7    7    7\n[8,]    8    9   10\n```\n\n\n:::\n:::\n\n\n\n​\n\nThe collection of all possible T statistics—one for each possible way to divide the $n$ observations into groups of size $n_1$, $\\mathcal{T} = \\{ T^{(1)}, T^{(2)}, \\dots, T^{(12870)} \\}$, can be generated using the following loop:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nT_perm <- numeric(M)\n\nfor (m in 1:M) {\n  # Create assignment vector for this permutation\n  Z_m <- numeric(n)\n  Z_m[all_perms[, m]] <- 1\n  \n  # Calculate test statistic for this permutation\n  X1_perm <- X[Z_m == 1]\n  X2_perm <- X[Z_m == 0]\n  T_perm[m] <- mean(X1_perm) - mean(X2_perm)\n}\n```\n:::\n\n\n\nWe can print the first three T static values as follows (we have computed them previously):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nT_perm[1:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10.75 13.50 12.00\n```\n\n\n:::\n:::\n\n\n\n​\n\nFinally, we create the histogram of the computed T statistics that is the exact permutation distribution. Each bar represents the number of times a particular range of $T$ values occurs across all possible permutations of the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(T_perm, breaks = 50, col = rgb(0.8, 1, 0.8), \n     main = \"Exact Permutation Distribution\",\n     xlab = \"Test Statistic T\", xlim = c(-15, 15))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\n**Step 5: Calculate the p-value**\n\nSince the $T^{obs} = 10.75 > 0$:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_value_exact <- (sum(T_perm >= T_obs) + sum(T_perm <= -T_obs)) / M\np_value_exact\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01787102\n```\n\n\n:::\n:::\n\n\n\n​\n\nThe extreme values of T statistic, which are unlikely under the null hypothesis, are highlighted in a light red color. The upper tail corresponds to $T \\ge T^{\\text{obs}}$, and the lower tail corresponds to $T \\le -T^{\\text{obs}}$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Histogram of permutation distribution with colored tails\nh <- hist(T_perm, breaks = 50, plot = FALSE)\ncolors <- ifelse(h$mids >= T_obs | h$mids <= -T_obs, \n                 rgb(1, 0, 0, 0.5), rgb(0.8, 1, 0.8))\n\nplot(h, col = colors, main = \"Exact Permutation Distribution\",\n     xlab = \"Test Statistic T\", xlim = c(-15, 15))\nabline(v = T_obs, col = \"darkred\", lwd = 2, lty = 2)\ntext(T_obs, max(h$counts) * 0.95,\n     bquote(T^{obs} == .(round(T_obs, 2))),\n     col = \"darkred\", cex = 1, font = 2, adj = c(0.5, 0))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\nor equivalently\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_value_exact <- mean(abs(T_perm) >= abs(T_obs))\np_value_exact\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01787102\n```\n\n\n:::\n:::\n\n\n\nThe corresponding p-value is $p = 0.0179$. Since $p < 0.05$, the observed test statistic lies in the extreme tails of the permutation distribution, indicating that the observed grouping is unusual under the null hypothesis. Therefore, we reject the null hypothesis at the 5% significance level, suggesting a statistically significant difference between the groups.\n\n​\n\nThe result can also be confirmed by using the **`oneway_test()`** function from the ***coin*** R package: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoin::oneway_test(X~factor(Z_obs), distribution=\"exact\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tExact Two-Sample Fisher-Pitman Permutation Test\n\ndata:  X by factor(Z_obs) (0, 1)\nZ = -2.2489, p-value = 0.01787\nalternative hypothesis: true mu is not equal to 0\n```\n\n\n:::\n:::\n\n\n\n​\n\n## Comparison to the classic two-sample T-test\n\nLet's formally apply the two-sample t-test to compare its p-value with that of the permutation test.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(X1, X2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  X1 and X2\nt = 2.6686, df = 13.108, p-value = 0.0192\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  2.054571 19.445429\nsample estimates:\nmean of x mean of y \n   64.125    53.375 \n```\n\n\n:::\n:::\n\n\n\nIn this example, the results are highly consistent. Both the exact Permutation Test ($p = 0.0179$) and the two-sample T-test ($p = 0.0192$) yield close p-values.\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}