[
  {
    "objectID": "teach.html",
    "href": "teach.html",
    "title": "Teaching",
    "section": "",
    "text": "I taught Biostatistics and R practical sessions for a few years at the Aristotle University of Thessaloniki in Greece.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "What’s New & Updated",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe Cross-Lagged Panel Model: A starting point for understanding longitudinal SEM\n\n\n\n\n\n\nStatistics\n\n\n\nThe cross-lagged panel model (CLPM) is a path analysis model that is typically estimated within the Structural Equation Modeling (SEM) framework for analyzing dynamic associations between variables measured at multiple time points.\n\n\n\n\n\nJan 10, 2026\n\n\nKonstantinos Bougioukas\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron: the first artificial neuron\n\n\n\n\n\n\nStatistics\n\n\n\nThe perceptron is one of the earliest and simplest models of an artificial neuron, introduced first by Warren McCulloch and Walter Pitts in 1943, and later by Frank Rosenblatt in 1958, as a computational approach inspired by biological neurons.\n\n\n\n\n\nJan 3, 2026\n\n\nKonstantinos Bougioukas\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-sample permutation test: How it works\n\n\n\n\n\n\nStatistics\n\n\n\nPermutation tests approximate the null distribution of a test statistic by repeatedly resampling the data in a way that is consistent with the null hypothesis.\n\n\n\n\n\nDec 15, 2025\n\n\nKonstantinos Bougioukas\n\n\n\n\n\n\n\n\n\n\n\n\nNormal Q-Q plot: A step-by-step approach\n\n\n\n\n\n\nvisualization\n\n\n\nA normal Q-Q plot allows you to visually assess whether your data follow a normal distribution.\n\n\n\n\n\nNov 30, 2025\n\n\nKonstantinos Bougioukas\n\n\n\n\n\n\n\n\n\n\n\n\nOur paper is a suggested reading from the University of Cambridge\n\n\n\n\n\n\nnews\n\n\n\nOur work on how to keep up with medical information is a suggested reading from the University of Cambridge\n\n\n\n\n\nFeb 9, 2024\n\n\nKonstantinos Bougioukas\n\n\n\n\n\n\n\n\n\n\n\n\nRecreation of the most popular covid-track plot using ggplot2\n\n\n\n\n\n\nvisualization\n\n\n\nFull recreation from scratch of the most popular covid-track plot using ggplot2 (Source of the original plot: Financial Times).\n\n\n\n\n\nFeb 7, 2024\n\n\nKonstantinos Bougioukas\n\n\n\n\n\n\n\n\n\n\n\n\namstar2Vis: An R package for presenting the critical appraisal of systematic reviews based on the items of AMSTAR 2\n\n\n\n\n\n\npaper\n\n\n\nWe expect the “amstar2Vis” package to be useful for authors of overviews of reviews of healthcare interventions and methodologists who wish to rate and present the overall confidence in the results of the included SRs.\n\n\n\n\n\nFeb 5, 2024\n\n\nKonstantinos Bougioukas, Paschalis Karakasis, Konstantinos Pamporis, Emmanouil Bouras, Anna-Bettina Haidich\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/2025-15-12-percepton/index.html",
    "href": "posts/2025-15-12-percepton/index.html",
    "title": "Perceptron: the first artificial neuron",
    "section": "",
    "text": "​"
  },
  {
    "objectID": "posts/2025-15-12-percepton/index.html#the-perceptron",
    "href": "posts/2025-15-12-percepton/index.html#the-perceptron",
    "title": "Perceptron: the first artificial neuron",
    "section": "The perceptron",
    "text": "The perceptron\nThe perceptron is one of the earliest and simplest models of an artificial neural network (McCulloch 1943; Rosenblatt 1958). Conceptually, it corresponds to a single artificial neuron that makes binary decisions. Despite its simplicity, the perceptron introduced the core ideas of weighted inputs, linear decision boundaries, and supervised learning for classification tasks—ideas that remain central to modern deep learning.\nAt its heart, the perceptron learns to separate data into two classes by finding a linear boundary that divides the input space. Each prediction is obtained by combining the inputs linearly and passing the result through a threshold function that produces a discrete output.\n\nHow it works\nThe Perceptron can be viewed as a simple directed graph, with edges connecting each input to the output and parameterized by weights. The output of the perceptron depends both on the values of the input nodes and on the weights associated with the incoming edges, which control how strongly each input influences the final decision.\n\n\n\nSchematic of a simple Perceptron model.\n\n\n​\n\n\nBasic set up and notation\nThe Perceptron takes multiple input features, multiplies each by an associated weight, sums these weighted inputs along with a bias term, and then applies a threshold decision (activation) function to determine the output class. This process can be described compactly using vector notation.\nSuppose our dataset consists of \\(P\\) input-target pairs\n\\[(\\mathbf{x_1}, y_1), (\\mathbf{x_2}, y_2), ..., (\\mathbf{x_P}, y_P)\\]\nor equivalently,\n\\[\\left\\{ \\left(\\mathbf{x}_{p},y_{p}\\right)\\right\\} _{p=1}^{P}\\]\nwhere \\(\\mathbf{x_p}\\) denotes the input feature vector for the \\(p-\\)observation and \\(y_p \\in \\{-1, +1\\}\\) represents the corresponding binary target label.\nEach input \\(\\mathbf{x_p}\\) is a column vector of length \\(N + 1\\), where the leading component is fixed to 1 in order to incorporate the bias term directly into the model:\n\\[\\mathbf{x_p} = \\begin{bmatrix} 1 \\\\ x_{1,\\ p} \\\\ x_{2,\\ p} \\\\\\vdots \\\\ x_{N,\\ P} \\end{bmatrix}\\]\nThe model parameters—the bias and feature weights—are collected into a single column vector:\n\\[\\mathbf{w} = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_N \\end{bmatrix}\\]\nThe target labels are represented by a vector \\(\\mathbf{y}\\) of length \\(P\\) :\n\\[\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_P \\end{bmatrix}\\]\n\n\nLinear Combination\nFor a single observation \\(p\\), the perceptron computes a net input \\(z_p\\) by taking the dot product of the weight vector and the input vector:\n\\[z_p = \\mathbf{w}^{\\mathsf{T}}\\mathbf{x}_p\\]\nwhere the superscript \\(\\mathsf{T}\\) denotes transposition.\nExpanding this expression makes the computation explicit:\n\\[\\begin{aligned}\nz_p &= \\begin{bmatrix} w_0 & w_1 & \\cdots & w_N \\end{bmatrix} \\begin{bmatrix} 1 \\\\ x_{1,\\ p} \\\\ x_{2,\\ p} \\\\\\vdots \\\\ x_{N,\\ P} \\end{bmatrix} \\\\\n&= (w_0 \\cdot 1) + (w_1 \\cdot x_{1,p}) + \\dots + (w_N \\cdot x_{N,P})  \\\\\n&= \\underbrace{\\enspace w_0 \\enspace}_{\\text{bias}} + \\underbrace{\\sum_{n=1}^{N} w_{n} \\cdot x_{n, p}}_{\\text{sum of weights}}\n\\end{aligned}\\]\nThus, the net input is simply a weighted sum of the input features plus a bias term.\n​\n\n\nActivation function and decision boundary\nIn the classic perceptron, the activation function \\(f(\\cdot)\\) is a step function, which maps the continuous net input to one of two discrete class labels. One possible choice is the sign function, which assigns a label depending on whether the net input is non-negative or negative, as follows:\n​\n\\[\\hat y_p = \\text{sign}(z_p) = \\begin{cases} 1 & \\text{if } z_p \\ge 0 \\\\ -1 & \\text{if } z_p &lt; 0 \\end{cases}\\]\n​\nThrough this simple rule, the perceptron partitions the input space into two regions separated by a linear decision boundary, classifying inputs on one side as \\(+1\\) and those on the other side as \\(-1\\), which is why it is considered a linear binary classifier; this decision boundary is defined as the input vectors for which the net input is zero, that is, \\(\\mathbf{w}^{\\mathsf{T}}\\mathbf{x}_p = 0\\).\n​\n\n\nThe weight update rule\nThe perceptron learning rule is an example of supervised training. For each input, the network produces an output (\\(\\hat y_p\\)), which is compared to the target value \\(y\\). The learning rule then updates the network’s weights of the network in order to move the network output closer to the target.\nLet \\(\\mathbf{w}^{\\text{old}}\\) denote the weight vector before the update, and \\(\\mathbf{w}^{\\text{new}}\\) denote the weight vector after the update. The learning rule is expressed as:\n\\[\\mathbf{w}^{\\text{new}} = \\mathbf{w}^{\\text{old}} + \\eta \\, (y_p - \\hat{y}_p) \\, \\mathbf{x}_p\\]\nwhere \\(\\eta &gt; 0\\) is the learning rate, controlling the step size of each update. The error term \\((y_p - \\hat{y}_p)\\) is a discrete scalar that determines the nature of the weight update.\nThe perceptron learning rule adjusts the weight vector in the direction that reduces the error for the specific example \\(p\\):\n\nCorrect prediction (\\(y_p = \\hat{y}_p\\)): Since \\((y_p - \\hat{y}_p) = 0\\), the update term is zero. No change is made, as the model’s current boundary already correctly classifies the point.\nIncorrect prediction (\\(y_p \\neq \\hat{y}_p\\)): The term \\((y_p - \\hat{y}_p)\\) becomes either \\(+2\\) or \\(-2\\). The weights are “nudged” in the direction of \\(y_p \\mathbf{x}_p\\), moving the decision boundary.\n\n​\n\n\nConvergence and separability\nTraining begins by assigning initial values to the network parameters. The perceptron learns by iteratively adjusting its weights to minimize classification errors. Learning proceeds in epochs, with each epoch representing a complete pass through the training dataset. During an epoch, the order of training examples may be randomly shuffled to improve generalization and prevent bias from the data sequence. Because weight updates are made incrementally after each individual example, multiple epochs are typically required for the decision boundary to gradually adjust and better separate the data. Whether this process eventually halts—achieving convergence—depends entirely on the linear separability of the training set.\nWhen the classes are linearly separable, meaning they can be perfectly divided by a straight line in two dimensions or a hyperplane in higher dimensions, the Perceptron Convergence Theorem guarantees that the algorithm will reach a finite weight vector that correctly classifies all training examples. At this point, the weights stabilize and no further updates occur. Conversely, if the data are not linearly separable, no such solution exists. The perceptron will continue updating its weights indefinitely, repeatedly shifting the decision boundary in an attempt to correct misclassifications. Fixing one misclassified point may cause another to become misclassified, producing oscillatory or “jittering” behavior. In practice, training on non-separable data is terminated after a fixed number of epochs to prevent unbounded computation.\n\n\n\n\n\n\n\n\n\nLinearly separable dataset.\n\n\n\n\n\n\n\nNon-separable dataset by a linear boundary.\n\n\n\n\n\n​"
  },
  {
    "objectID": "posts/2025-15-12-percepton/index.html#perceptron-example-with-r",
    "href": "posts/2025-15-12-percepton/index.html#perceptron-example-with-r",
    "title": "Perceptron: the first artificial neuron",
    "section": "Perceptron example with R",
    "text": "Perceptron example with R\n\nData Preparation\nDataset\nFor this example, we’ll work with the well-known Iris dataset, one of the most commonly used datasets in machine learning. To simplify the task into a binary classification problem, we’ll focus on just two species: Iris versicolor and Iris virginica. Using the measurements of sepal length and petal length, we’ll explore how these features can be used to distinguish between the two species.\n\n\n\nSepal and peta lengths of iris dataset.\n\n\n\n# load the packages \nlibrary(tidyverse)\nlibrary(gganimate)\n\n\n# load the dataset\ndata(iris)\ndf &lt;- iris[iris$Species != \"setosa\", ] \ndf$y &lt;- ifelse(df$Species == \"versicolor\", 1, -1)\n\n​\nFeature Scaling\nStandardization ensures all features have mean = 0 and standard deviation = 1. This helps the Perceptron converge faster and prevents features with larger scales from dominating the learning process.\n\n# Feature Scaling\ndf_scaled &lt;- df\ndf_scaled[, 1:4] &lt;- scale(df[, 1:4])\ndf_scaled_final &lt;- df_scaled[, c(\"Sepal.Length\", \"Petal.Length\", \"y\")]\n\n\n# Initial setup\nX &lt;- as.matrix(df_scaled_final[, 1:2])\nX_augmented &lt;- cbind(1, X) # leading 1 for bias\ny_vector &lt;- df_scaled_final$y\n\nThe augmented matrix includes a column of 1s, which allows us to incorporate the bias term \\(w_o\\) directly into the weight vector.\n​\n\n\nImplement the Perceptron Algorithm\nSign Activation Function\n\nsign_func &lt;- function(z) {\n  ifelse(z &gt;= 0, 1, -1)\n}\n\n​\nPerceptron Training Function\nThe perceptron algorithm repeatedly cycles through training examples in a randomized order, iteratively adjusting weights only when a misclassification occurs. This process continues until the model achieves convergence (zero errors) or reaches a preset maximum number of epochs. Because the updates are incremental, a single data point may be processed multiple times across different epochs, gradually nudging the decision boundary until it successfully partitions the classes with a valid separating hyperplane.\n\nfit_perceptron &lt;- function(X_aug, y, eta = 0.05, n_iter = 10) {\n  n_features &lt;- ncol(X_aug)\n  n_obs &lt;- nrow(X_aug)\n  \n  # Initialize weights\n  set.seed(842)\n  w &lt;- rnorm(n_features, mean = 0, sd = 0.1) \n  \n  history_list &lt;- list()\n  epoch_errors &lt;- numeric(n_iter)\n  step_total &lt;- 0\n  \n  for (epoch in 1:n_iter) {\n    # --- Shuffle data at the start of every epoch ---\n    indices &lt;- sample(n_obs)\n    X_shuffled &lt;- X_aug[indices, ]\n    y_shuffled &lt;- y[indices]\n    \n    error_count &lt;- 0\n    \n    for (p in 1:n_obs) {\n      step_total &lt;- step_total + 1\n      x_p &lt;- X_shuffled[p, ]\n      y_p &lt;- y_shuffled[p]\n      \n      # Linear Combination\n      z_p &lt;- as.numeric(t(w) %*% x_p)\n      y_hat &lt;- sign_func(z_p)\n      \n      is_error &lt;- (y_hat != y_p)\n      if (is_error) {\n        # Perceptron Update Rule\n        w &lt;- w + eta * (y_p - y_hat) * x_p\n        error_count &lt;- error_count + 1\n      }\n      \n      # Calculate accuracy on full dataset for tracking\n      predictions &lt;- sign_func(X_aug %*% w)\n      current_accuracy &lt;- sum(predictions == y) / length(y) * 100\n      \n      # Record history\n      history_list[[step_total]] &lt;- data.frame(\n        Step = step_total, \n        Epoch = epoch,\n        Iteration = p,\n        w0 = w[1], w1 = w[2], w2 = w[3],\n        x1_val = x_p[2], x2_val = x_p[3], # Tracks the specific point being tested\n        is_error = is_error,\n        current_acc = current_accuracy\n      )\n    }\n    \n    epoch_errors[epoch] &lt;- error_count\n    cat(sprintf(\"Epoch %d: %d errors\\n\", epoch, error_count))\n    \n    if (error_count == 0) {\n      cat(sprintf(\"--- Converged early at Epoch %d ---\\n\", epoch))\n      break\n    }\n  }\n  \n  return(list(\n    weights = w, \n    errors = epoch_errors[1:epoch], \n    history = bind_rows(history_list)\n  ))\n}\n\n​\nLet’s train the perceptron model for 30 epochs and record how many times the weights are updated during each full pass through the training data in each Epoch (Epoch error).\n\n# Training\nresults &lt;- fit_perceptron(X_augmented, y_vector, n_iter = 30)\n\nEpoch 1: 14 errors\nEpoch 2: 12 errors\nEpoch 3: 8 errors\nEpoch 4: 11 errors\nEpoch 5: 13 errors\nEpoch 6: 9 errors\nEpoch 7: 8 errors\nEpoch 8: 7 errors\nEpoch 9: 3 errors\nEpoch 10: 8 errors\nEpoch 11: 10 errors\nEpoch 12: 8 errors\nEpoch 13: 7 errors\nEpoch 14: 6 errors\nEpoch 15: 10 errors\nEpoch 16: 9 errors\nEpoch 17: 10 errors\nEpoch 18: 8 errors\nEpoch 19: 6 errors\nEpoch 20: 4 errors\nEpoch 21: 7 errors\nEpoch 22: 9 errors\nEpoch 23: 9 errors\nEpoch 24: 7 errors\nEpoch 25: 8 errors\nEpoch 26: 5 errors\nEpoch 27: 8 errors\nEpoch 28: 11 errors\nEpoch 29: 7 errors\nEpoch 30: 5 errors\n\nhistory_df &lt;- results$history\nerrors &lt;- results$errors\nw_final &lt;- results$weights\n\n\n# Epoch error plot\nplot(1:length(errors), errors, type = \"o\", pch = 16, \n     col = \"blue\", lwd = 2, cex = 1.5,\n     main = \"Learning Curve (30 epochs)\", xlab = \"Epoch\", ylab = \"Errors\")\n\n\n\n\nPerceptron learning curve showing epoch error\n\n\n\n\nThe algorithm is not converging - the errors are oscillating between 3 and 14, never reaching zero. Correcting one misclassification inevitably leads to another, causing a never-ending cycle of updates. This behavior indicates that the data are not perfectly linearly separable.\n​\nSimulate the Learning Process\nWe’ll simulate the training process and compute current accuracy, defined as the percentage of the entire dataset that is correctly classified by the weights at any given moment.\n\n\n\n\n\n\n\n\n\n​\nThe final decision boundary is the geometric representation of our learned classifier for the peak current accuracy.\n\n\n\n\n\nFinal decision boundary separating Versicolor and Virginica\n\n\n\n\nWe observe 96% accuracy in Epoch 4, which means that at that specific point the model correctly classified 96 out of 100 points and only 4 points were misclassified (i.e., fell on the wrong side of the decision boundary)."
  },
  {
    "objectID": "posts/2025-01-10-clpm/index.html",
    "href": "posts/2025-01-10-clpm/index.html",
    "title": "The Cross-Lagged Panel Model: A starting point for understanding longitudinal SEM",
    "section": "",
    "text": "​"
  },
  {
    "objectID": "posts/2025-01-10-clpm/index.html#the-cross-lagged-panel-model",
    "href": "posts/2025-01-10-clpm/index.html#the-cross-lagged-panel-model",
    "title": "The Cross-Lagged Panel Model: A starting point for understanding longitudinal SEM",
    "section": "The Cross-Lagged Panel Model",
    "text": "The Cross-Lagged Panel Model\nThe Cross-Lagged Panel Model (CLPM) is a type of path analysis within the discrete-time structural equation modeling (SEM) framework used to analyze panel (longitudinal) data where the same variables are measured repeatedly across multiple time points (Kenny 1975). We will explore the traditional CLPM in this analysis, recognizing that while it has important limitations due to its relative simplicity (Hamaker 2015), it remains a useful starting point for understanding fundamental concepts in longitudinal modeling.\n\nBasic concepts\nFigure 1 shows a model with three time points (\\(T1\\), \\(T2\\), and \\(T3\\)), often called waves. The squares represent the observed variables, \\(X\\) and \\(Y\\), at each time point. Single-headed arrows indicate regression effects (directional effects), and double-headed arrows indicate correlations or (co)variances.\n​\n\n\n\n\n\n\nFigure 1: Schematic of a bivariate CLPM for three waves and two observed variables.\n\n\n\n​\nMore specific, a CLPM typically includes:\n\nThe single-headed horizontal arrows (labeled \\(a\\) and \\(b\\)) represent autoregressive effects, also known as stability paths. Specifically, paths \\(a_1\\) and \\(a_2\\) indicate how well variable \\(X\\) at one time point predicts its own value at the next wave. Similarly, paths \\(b_1\\) and \\(b_2\\) indicate the longitudinal stability of variable \\(Y\\) over time. Notably, higher coefficients in these paths suggest that the variable is highly stable and doesn’t change much between waves.\nThe single-headed diagonal arrows represent the core of the model: the cross-lagged effects. Based on the specific configuration of this model, paths \\(h_1\\) and \\(h_2\\) show the influence of \\(X\\) on future changes in \\(Y\\). Conversely, paths \\(g_1\\) and \\(g_2\\) show the influence of \\(Y\\) on future changes in \\(X\\). These pathways make it possible to evaluate the “directional” influence between the two variables over time.\nWithin-time correlations: associations between variables at the same measurement occasion.\n\n​\n\n\nDirect paths and indirect effects\nTo further detail the mechanics of the cross-lagged panel model, the associations between waves can be mathematically expressed through direct paths and indirect effects (mediation paths). These equations illustrate how the variables at a later time point are a function of the variables from the preceding wave.\nDirect path equations\n\\[X_{T2} = (intercept) + a_1\\cdot X_{T1} + g_1 \\cdot Y_{T1}\\] \\[Y_{T2} = (intercept) + b_1\\cdot Y_{T1} + h_1 \\cdot X_{T1}\\]\n\\[X_{T3} = (intercept) + a_2\\cdot X_{T2} + g_2 \\cdot Y_{T2}\\] \\[Y_{T3} = (intercept) + b_2\\cdot y_{T2} + h_2 \\cdot X_{T2}\\]\nIndirect effects (Mediation Paths)\n\\[X_{T1} \\Rightarrow X_{T2} \\Rightarrow Y_{T3}: a_1 \\cdot h_2\\] \\[X_{T1} \\Rightarrow Y_{T2} \\Rightarrow Y_{T3}: h_1 \\cdot b_2\\]\n\\[X_{T1} \\Rightarrow Y_{T2} \\Rightarrow X_{T3}: h_1 \\cdot g_2\\]\n\\[Y_{T1} \\Rightarrow X_{T2} \\Rightarrow X_{T3}: g_1 \\cdot a_2\\]\n\\[Y_{T1} \\Rightarrow Y_{T2} \\Rightarrow X_{T3}: b_1 \\cdot g_2\\]\n\\[Y_{T1} \\Rightarrow X_{T2} \\Rightarrow Y_{T3}: g_1 \\cdot h_2\\] ​\nAdditionally, the autoregressive indirect effects are:\n\\[X_{T1} \\Rightarrow X_{T2} \\Rightarrow X_{T3}: a_1 \\cdot a_2\\] \\[Y_{T1} \\Rightarrow Y_{T2} \\Rightarrow Y_{T3}: b_1 \\cdot b_2\\]\n​"
  },
  {
    "objectID": "posts/2025-01-10-clpm/index.html#clpm-with-r",
    "href": "posts/2025-01-10-clpm/index.html#clpm-with-r",
    "title": "The Cross-Lagged Panel Model: A starting point for understanding longitudinal SEM",
    "section": "CLPM with R",
    "text": "CLPM with R\n\nData Preparation\nDataset\nFor this example, we examine the bidirectional association of weight bias internalization (WBIS) with BMI over time. Using a longitudinal design, data were collected from 925 participants across three distinct time points, or waves (\\(T_1, T_2, \\text{and } T_3\\)) (Figure 2). WBIS is defined as the internalization of negative stereotypes and beliefs about one’s own body weight.\n​\n\n\n\n\n\n\nFigure 2: Schematic of cross-lagged panel model (CLPM) assessing the association of weight bias internalization (WBIS) with Body Mass Index (BMI) over three timepoints.\n\n\n\n​\n\n# Load necessary libraries\nlibrary(lavaan)\nlibrary(tidySEM)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n# load the dataset\nlibrary(readr)\ndat &lt;- read_csv(\"WBIS_BMI.csv\")\ndat\n\n# A tibble: 925 × 6\n   WBIS_T1 WBIS_T2 WBIS_T3 BMI_T1 BMI_T2 BMI_T3\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1    4.09    4.27    3.91   28.0   28.7   28.7\n 2    6       5.82    6.55   20.3   20.3   19.9\n 3    1.82    1.18    1      26.0   24.1   23.1\n 4    6.64    6       6.27   26.5   26.5   26.5\n 5    3.27    4.09    4.27   25.8   26.4   27.1\n 6    2.91    2.27    3.55   24.4   24.0   24.5\n 7    1.82    2.55    2.18   30.8   31.4   31.1\n 8    5.73    5.73    6.18   26.9   25.8   26.3\n 9    5.91    5.91    6.36   38.9   38.4   40.7\n10    4.27    5       4.18   26.6   26.2   25.8\n# ℹ 915 more rows\n\n\n​\n\n\nCross-Lagged Panel Model\nModel syntax\nThe following syntax defines a 3-Wave Cross-Lagged Panel Model (CLPM) to examine the longitudinal association between Weight Bias Internalization (WBIS) and BMI. The code organizes the analysis into three primary dynamics:\n\nregressions (denoted by ~) represent both autoregressive paths, which measure the stability of a variable over time, and cross-lagged paths, which test if one variable predicts subsequent changes in the other.\nwithin-wave correlations (denoted by ~~) account for the shared variance between WBIS and BMI at each time point.\nindirect effects (using := operator) to estimate longitudinal mediation pathways by calculating whether an initial measurement at Time 1 influences an outcome at Time 3 through a mediation effect at Time 2.\n\n\n# --- Define the Model ---\n\nmy_model &lt;- '\n  # Regressions (Direct Paths)\n  WBIS_T2 ~ a1*WBIS_T1 + g1*BMI_T1\n  BMI_T2  ~ b1*BMI_T1  + h1*WBIS_T1\n  \n  WBIS_T3 ~ a2*WBIS_T2 + g2*BMI_T2\n  BMI_T3  ~ b2*BMI_T2  + h2*WBIS_T2\n\n  # Covariances (Within-Wave Associations)\n  WBIS_T1 ~~ BMI_T1\n  WBIS_T2 ~~ BMI_T2\n  WBIS_T3 ~~ BMI_T3\n\n  # Indirect Effects\n  wb1_wb2_bm3 := a1 * h2\n  wb1_bm2_bm3 := h1 * b2\n  wb1_bm2_wb3 := h1 * g2\n  \n  bm1_wb2_wb3 := g1 * a2\n  bm1_bm2_wb3 := b1 * g2\n  bm1_wb2_bm3 := g1 * h2\n  \n  wb1_wb2_wb3 := a1 * a2\n  bm1_bm2_bm3 := b1 * b2\n'\n\n​\nsem() function\nThe sem() function from the lavaan package is used to fit the model to the dataset by estimating the associations defined in the model syntax. Within this function, fixed.x = FALSE ensures that the Time 1 variables are treated as observed rather than fixed, which is necessary for estimating baseline correlations, and meanstructure = TRUE includes intercepts in the estimation to account for the average levels of \\(WBIS\\) and \\(BMI\\) across waves.\n\n# --- Fit the Model ---\nfit &lt;- sem(my_model, data = dat, fixed.x = FALSE, meanstructure = TRUE)\n\n​\nOnce the model is fitted, the summary() function is used to evaluate the results. By including the standardized = TRUE argument, the output displays a Std.all column containing the standardized coefficients (Beta weights), which are essential for comparing the relative strength of the paths between WBIS and BMI. Additionally, fit.measures = TRUE generates the necessary indices (such as CFI, TLI, and RMSEA) required to report the model’s overall “goodness-of-fit”.\n\n# --- View Results ---\nsummary(fit, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)\n\nlavaan 0.6-20 ended normally after 38 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n  Number of observations                           925\n\nModel Test User Model:\n                                                      \n  Test statistic                               108.605\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              6985.667\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.985\n  Tucker-Lewis Index (TLI)                       0.944\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10753.771\n  Loglikelihood unrestricted model (H1)     -10699.468\n                                                      \n  Akaike (AIC)                               21553.541\n  Bayesian (BIC)                             21664.626\n  Sample-size adjusted Bayesian (SABIC)      21591.581\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.168\n  90 Percent confidence interval - lower         0.142\n  90 Percent confidence interval - upper         0.196\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.016\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  WBIS_T2 ~                                                             \n    WBIS_T1   (a1)    0.820    0.018   44.356    0.000    0.820    0.830\n    BMI_T1    (g1)    0.011    0.005    2.477    0.013    0.011    0.046\n  BMI_T2 ~                                                              \n    BMI_T1    (b1)    0.920    0.011   80.578    0.000    0.920    0.931\n    WBIS_T1   (h1)    0.146    0.047    3.142    0.002    0.146    0.036\n  WBIS_T3 ~                                                             \n    WBIS_T2   (a2)    0.846    0.019   45.219    0.000    0.846    0.837\n    BMI_T2    (g2)    0.010    0.005    2.217    0.027    0.010    0.041\n  BMI_T3 ~                                                              \n    BMI_T2    (b2)    0.971    0.011   91.473    0.000    0.971    0.948\n    WBIS_T2   (h2)    0.094    0.043    2.180    0.029    0.094    0.023\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  WBIS_T1 ~~                                                            \n    BMI_T1            3.568    0.343   10.417    0.000    3.568    0.365\n .WBIS_T2 ~~                                                            \n   .BMI_T2            0.197    0.055    3.586    0.000    0.197    0.119\n .WBIS_T3 ~~                                                            \n   .BMI_T3            0.148    0.050    2.975    0.003    0.148    0.098\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .WBIS_T2           0.388    0.118    3.301    0.001    0.388    0.254\n   .BMI_T2            1.867    0.296    6.296    0.000    1.867    0.299\n   .WBIS_T3           0.268    0.119    2.251    0.024    0.268    0.174\n   .BMI_T3            0.615    0.276    2.228    0.026    0.615    0.096\n    WBIS_T1           3.356    0.051   65.897    0.000    3.356    2.167\n    BMI_T1           26.836    0.208  129.164    0.000   26.836    4.247\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .WBIS_T2           0.657    0.031   21.506    0.000    0.657    0.281\n   .BMI_T2            4.175    0.194   21.506    0.000    4.175    0.107\n   .WBIS_T3           0.650    0.030   21.506    0.000    0.650    0.272\n   .BMI_T3            3.491    0.162   21.506    0.000    3.491    0.085\n    WBIS_T1           2.399    0.112   21.506    0.000    2.399    1.000\n    BMI_T1           39.930    1.857   21.506    0.000   39.930    1.000\n\nR-Square:\n                   Estimate\n    WBIS_T2           0.719\n    BMI_T2            0.893\n    WBIS_T3           0.728\n    BMI_T3            0.915\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    wb1_wb2_bm3       0.077    0.036    2.177    0.029    0.077    0.019\n    wb1_bm2_bm3       0.142    0.045    3.140    0.002    0.142    0.034\n    wb1_bm2_wb3       0.001    0.001    1.811    0.070    0.001    0.001\n    bm1_wb2_wb3       0.009    0.004    2.473    0.013    0.009    0.039\n    bm1_bm2_wb3       0.009    0.004    2.216    0.027    0.009    0.038\n    bm1_wb2_bm3       0.001    0.001    1.636    0.102    0.001    0.001\n    wb1_wb2_wb3       0.693    0.022   31.665    0.000    0.693    0.695\n    bm1_bm2_bm3       0.894    0.015   60.464    0.000    0.894    0.882\n\n\n​\n\n\nUnderstanding the output from lavaan\nWhile the lavaan output is notoriously comprehensive and can be overwhelming at first glance, it is structured logically to provide a complete view of our model’s performance. It is helpful to view the output in three distinct stages:\nThe global model fit\n\nfitMeasures(fit, c(\"cfi\", \"tli\", \"rmsea\", \"srmr\"))\n\n  cfi   tli rmsea  srmr \n0.985 0.944 0.168 0.016 \n\n\nAccording to Hu and Bentler (1999), an acceptable fit is achieved when CFI \\(\\ge\\) 0.95 and SRMR \\(\\le\\) 0.08—a guideline known as the combination rule. This recommendation, however, remains a topic of debate, as these cut-offs can be overly restrictive for complex models or those with smaller sample sizes.\n​\nDirect path equations\nUsing the parameterEstimates() function allows us to extract the direct path parameters of the model.\n\n# Pull the estimates\nestimates &lt;- parameterEstimates(fit)\n\n​\nThe intercepts\nThe following code extracts the intercept for each equation:\n\n# intercepts\nsubset(estimates, op == \"~1\")[, c(\"lhs\", \"est\")]\n\n       lhs    est\n18 WBIS_T2  0.388\n19  BMI_T2  1.867\n20 WBIS_T3  0.268\n21  BMI_T3  0.615\n22 WBIS_T1  3.356\n23  BMI_T1 26.836\n\n\n​\nThe regression coefficients\nWe obtain the regression coefficients (the \\(B\\) weights) from the unstandardized est column:\n\n# regressions\nsubset(estimates, op == \"~\")\n\n      lhs op     rhs label   est    se      z pvalue ci.lower ci.upper\n1 WBIS_T2  ~ WBIS_T1    a1 0.820 0.018 44.356  0.000    0.783    0.856\n2 WBIS_T2  ~  BMI_T1    g1 0.011 0.005  2.477  0.013    0.002    0.020\n3  BMI_T2  ~  BMI_T1    b1 0.920 0.011 80.578  0.000    0.898    0.942\n4  BMI_T2  ~ WBIS_T1    h1 0.146 0.047  3.142  0.002    0.055    0.238\n5 WBIS_T3  ~ WBIS_T2    a2 0.846 0.019 45.219  0.000    0.809    0.883\n6 WBIS_T3  ~  BMI_T2    g2 0.010 0.005  2.217  0.027    0.001    0.019\n7  BMI_T3  ~  BMI_T2    b2 0.971 0.011 91.473  0.000    0.951    0.992\n8  BMI_T3  ~ WBIS_T2    h2 0.094 0.043  2.180  0.029    0.010    0.179\n\n\n​ ​\nRegression equations\nBased on the unstandardized parameter estimates, the longitudinal dynamics between Weight Bias Internalization (WBIS) and Body Mass Index (BMI) are defined by the following regression equations.\nWave 2 Predictions:\n\\[WBIS\\_T2 = 0.388 + 0.820 \\cdot WBIS\\_{T1} + 0.011 \\cdot BMI\\_{T1}\\]\n\\[BMI\\_{T2} = 1.867 + 0.920 \\cdot BMI\\_{T1} + 0.146 \\cdot WBIS\\_{T1}\\]\nWave 3 Predictions:\n\\[WBIS\\_{T3} = 0.268 + 0.846 \\cdot WBIS\\_{T2} + 0.010 \\cdot BMI\\_{T2}\\]\n\\[BMI\\_{T3} = 0.615 + 0.971 \\cdot BMI\\_{T2} + 0.094 \\cdot WBIS\\_{T2}\\]\n​\nNow, let’s interpret the BMI Wave 3 Prediction:\n\\[BMI\\_{T3} = 0.615 + 0.971 \\cdot BMI\\_{T2} + 0.094 \\cdot WBIS\\_{T2}\\]\nThe intercept is the constant or the “starting value” for \\(BMI_{T3}\\) when all other predictors in the equation are zero. It is the base level for BMI at Wave 3.\nStability (\\(b_2 = 0.971\\)): For every 1-unit increase in BMI at Wave 2, BMI at Wave 3 is predicted to increase by 0.971 units, adjusting for Weight Bias Internalization at wave 2.\nCross-lag (\\(h_2 = 0.094\\)): This is the most important coefficient for testing our theoretical model, as it represents the effect of one variable on a different variable over time. For every 1-unit increase in the Weight Bias Internalization Scale (WBIS) at Wave 2, BMI at Wave 3 is predicted to increase by 0.094 units, adjusting the previous BMI.\n​\nIndirect effects\n\n# Pull only the indirect effects\nsubset(estimates, op == \":=\")\n\n           lhs op   rhs       label   est    se      z pvalue ci.lower ci.upper\n24 wb1_wb2_bm3 := a1*h2 wb1_wb2_bm3 0.077 0.036  2.177  0.029    0.008    0.147\n25 wb1_bm2_bm3 := h1*b2 wb1_bm2_bm3 0.142 0.045  3.140  0.002    0.053    0.231\n26 wb1_bm2_wb3 := h1*g2 wb1_bm2_wb3 0.001 0.001  1.811  0.070    0.000    0.003\n27 bm1_wb2_wb3 := g1*a2 bm1_wb2_wb3 0.009 0.004  2.473  0.013    0.002    0.017\n28 bm1_bm2_wb3 := b1*g2 bm1_bm2_wb3 0.009 0.004  2.216  0.027    0.001    0.018\n29 bm1_wb2_bm3 := g1*h2 bm1_wb2_bm3 0.001 0.001  1.636  0.102    0.000    0.002\n30 wb1_wb2_wb3 := a1*a2 wb1_wb2_wb3 0.693 0.022 31.665  0.000    0.650    0.736\n31 bm1_bm2_bm3 := b1*b2 bm1_bm2_bm3 0.894 0.015 60.464  0.000    0.865    0.923\n\n\nAll four indirect effects are statistically significant (p&lt;0.05).\n​"
  },
  {
    "objectID": "posts/2025-01-10-clpm/index.html#path-diagram",
    "href": "posts/2025-01-10-clpm/index.html#path-diagram",
    "title": "The Cross-Lagged Panel Model: A starting point for understanding longitudinal SEM",
    "section": "Path diagram",
    "text": "Path diagram\nIn Structural Equation Modeling (SEM) frmamwork and path analysis, a graph—often called a path diagram—is not just an illustration; it is a mathematical map of the theory. Using the tidySEM and lavaan package in R allows us to translate these mathematical theories into informative visualizations. We will visualize both unstandardized and standardized coefficients.\nGraph with Unstandardized Coefficients\n\n# --- Visualization with tidySEM: Unstandardized estimates ---\n\n# Define the spatial layout\nlay &lt;- get_layout(\n  \"WBIS_T1\", \"WBIS_T2\", \"WBIS_T3\",\n  \"BMI_T1\",  \"BMI_T2\",  \"BMI_T3\", \n  rows = 2\n)\n\n# --- PREPARE GRAPH DATA ---\nres &lt;- table_results(fit, standardized = FALSE)\ngraph_data &lt;- prepare_graph(fit, layout = lay, results = res)\n\n# --- GET RAW P-VALUES FROM SEM() ---\n# Align lavaan terminology (lhs/rhs) with tidySEM terminology (from/to)\nraw_pvals &lt;- parameterEstimates(fit) %&gt;%\n  filter(op == \"~\" | op == \"~~\") %&gt;%\n  mutate(from = ifelse(op == \"~\", rhs, lhs),\n         to = ifelse(op == \"~\", lhs, rhs)) %&gt;%\n  select(from, to, op, p_raw = pvalue)\n\n# --- FORMAT EDGES ---\ngraph_data$edges &lt;- graph_data$edges %&gt;%\n  left_join(raw_pvals, by = c(\"from\", \"to\", \"op\")) %&gt;%\n  mutate(\n    # Recalculate stars based on precise p_raw\n    new_stars = case_when(\n      from == to ~ \"\",                    # No stars for variances\n      is.na(p_raw) ~ \"\",                  # Fallback for paths not found\n      p_raw &lt; 0.001 ~ \"***\",\n      p_raw &lt; 0.01  ~ \"**\",\n      p_raw &lt; 0.05  ~ \"*\",                \n      TRUE          ~ \"\"\n    ),\n    \n    # Check significance for styling\n    is_sig = !is.na(p_raw) & p_raw &lt; 0.05,\n    is_cov = !is.na(curvature) & curvature != 0,\n    is_variance = from == to,\n    \n    # LABEL: Use Rounded Est + new Stars for paths; only Est for variances\n    label = ifelse(is_variance, \n                   round(as.numeric(est), 2), \n                   paste0(round(as.numeric(est), 3), new_stars)),\n    \n    # STYLE: Colors based on precise p-value\n    color = ifelse(is_sig | is_variance, \"black\", \"grey75\"),\n    linewidth = 0.5,\n    linetype = ifelse(is_cov, 2, 1)\n  )\n\n# --- NODE LABELING  ---\nintercept_table &lt;- parameterEstimates(fit) |&gt; filter(op == \"~1\")\nint_lookup &lt;- setNames(intercept_table$est, intercept_table$lhs)\nr2_values &lt;- inspect(fit, \"rsquare\")\n\ngraph_data$nodes &lt;- graph_data$nodes |&gt; \n  mutate(\n    node_width = 1.8, node_height = 1.2,\n    label = sapply(name, function(x) {\n      val_est &lt;- if(x %in% names(int_lookup)) round(int_lookup[x], 2) else NA\n      if (is.na(val_est)) return(gsub(\"_\", \" \", x))\n      if (grepl(\"T1\", x)) {\n        paste0(gsub(\"_\", \" \", x), \"\\nMean: \", val_est)\n      } else {\n        val_r2 &lt;- if(x %in% names(r2_values)) round(r2_values[x], 2) else 0\n        paste0(gsub(\"_\", \" \", x), \"\\nInt: \", val_est, \"\\nR²: \", val_r2)\n      }\n    })\n  )\n\n# --- FINAL PLOT ---\nplot(graph_data) + \n  labs(\n    title = \"Unstandardized Regression Coefficients\",\n    subtitle = \"Significant paths highlighted with asterisks\n    (*p &lt; .05, **p &lt; .01, ***p &lt; .001)\",\n    caption = \"Solid lines = Regressions; Dashed lines = Covariances; \n    Circles = Variances.\"\n  )\n\n\n\n\n\n\n\n\n​\nGraph with Standardized Coefficients\n\n# Prepare the graph data\ngraph_data_std &lt;- prepare_graph(fit, layout = lay, standardized = TRUE)\n\n# Get high-precision p-values from lavaan\n# IMPORTANT: In lavaan Y ~ X, lhs is Y (to) and rhs is X (from).\nraw_pvals &lt;- parameterEstimates(fit) |&gt;\n  filter(op == \"~\" | op == \"~~\") |&gt; \n  mutate(from = ifelse(op == \"~\", rhs, lhs),\n         to = ifelse(op == \"~\", lhs, rhs)) |&gt;\n  select(from, to, p_raw = pvalue)\n\n# Merge and create the label\ngraph_data_std$edges &lt;- graph_data_std$edges |&gt;\n  left_join(raw_pvals, by = c(\"from\", \"to\")) |&gt;\n  mutate(\n    # Create the stars based on the high-precision p_raw\n    new_stars = case_when(\n      from == to ~ \"\",                    # No stars for variances\n      is.na(p_raw) ~ \"\",                  # Handle missing p-values\n      p_raw &lt; 0.001 ~ \"***\",\n      p_raw &lt; 0.01  ~ \"**\",\n      p_raw &lt; 0.05  ~ \"*\",                \n      TRUE          ~ \"\"\n    ),\n    # Overwrite the label with rounded estimate + precise stars\n    label = paste0(round(as.numeric(est_std), 2), new_stars)\n  )\n\n# Standard Node Formatting\ngraph_data_std$nodes &lt;- graph_data_std$nodes |&gt; \n  mutate(\n    node_width = 1.5, node_height = 1.0,\n    label = sapply(name, function(x) {\n      clean_name &lt;- gsub(\"_\", \" \", x)\n      if (grepl(\"T1\", x)) return(clean_name)\n      val_r2 &lt;- if(x %in% names(r2_values)) round(r2_values[x], 2) else 0\n      paste0(clean_name, \"\\nR²: \", val_r2)\n    })\n  )\n\n# Plot\nplot(graph_data_std) + \nlabs(\n    title = \"Standardized Regression Coefficients\",\n    subtitle = \"Significant paths highlighted with asterisks\n    (*p &lt; .05, **p &lt; .01, ***p &lt; .001)\",\n    caption = \"Solid lines = Regressions; Dashed lines = Covariances; \n    Circles = Variances.\"\n  )"
  },
  {
    "objectID": "posts/2024-02-07-covid/index.html",
    "href": "posts/2024-02-07-covid/index.html",
    "title": "Recreation of the most popular covid-track plot using ggplot2",
    "section": "",
    "text": "Coronavirus deaths in Italy, Spain, the UK and US are increasing more rapidly than they did in China\n\n\n\nReferences\n\n\n\n\n\n\n     \n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "R packages",
    "section": "",
    "text": "I have contributed to the following three R packages (all available open source at github)."
  },
  {
    "objectID": "packages.html#overlap-of-primary-studies-in-overviews-of-reviews",
    "href": "packages.html#overlap-of-primary-studies-in-overviews-of-reviews",
    "title": "R packages",
    "section": "Overlap of primary studies in overviews of reviews",
    "text": "Overlap of primary studies in overviews of reviews\n\n\n\n\n\nccaR\nccaR provides functions for assessing and depicting primary study overlap across multiple reviews. The functions may be useful for methodologists and overview authors in exploring and communicating the degree of overlap in overview of reviews."
  },
  {
    "objectID": "packages.html#critical-appraisal-of-systematic-reviews-based-on-the-items-of-amstar-2",
    "href": "packages.html#critical-appraisal-of-systematic-reviews-based-on-the-items-of-amstar-2",
    "title": "R packages",
    "section": "Critical appraisal of systematic reviews based on the items of AMSTAR 2",
    "text": "Critical appraisal of systematic reviews based on the items of AMSTAR 2\n\n\n\n\n\namstar2Vis\nWe present a free, open-source R package called “amstar2Vis” that provides easy-to-use functions for presenting the critical appraisal of systematic reviews in pertinent tables and graphs based on the items of AMSTAR 2 checklist."
  },
  {
    "objectID": "packages.html#discrete-color-scales-from-music-album-covers",
    "href": "packages.html#discrete-color-scales-from-music-album-covers",
    "title": "R packages",
    "section": "Discrete color scales from music album covers",
    "text": "Discrete color scales from music album covers\n\n\n\n\n\nmusicolor\nDiscrete color scales generated from music album covers ready to be used in ggplot visualizations. Discrete colors from Dire Straits, Pink Floyd, and many other album covers."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Konstantinos I. Bougioukas MSc, PhD",
    "section": "",
    "text": "E-mail\n  \n  \n    \n     Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     ORCID\n  \n  \n    \n     Scopus\n  \n  \n    \n     Google Scholar\n  \n  \n    \n     Research Gate\n  \n\n  \n  \n\n\nI am a research methodologist and data analyst specializing in evidence synthesis, an applied statistician, and an independent data visualization enthusiast based in Greece. Driven by a profound interest in meta-research within the medical domain, social sciences, and public health, I am deeply engaged in exploring avenues to enhance the integrity, rigor, and reproducibility of data analysis and visualization. Recently, I have also begun exploring the applications of artificial intelligence in healthcare and evidence synthesis tasks, aiming to optimize decision-making processes and research outcomes.\nI have extensive experience teaching Statistics and leading R practical sessions as an Adjunct Lecturer at the Aristotle University of Thessaloniki. Complementing my teaching role, I authored the free, online version of the textbook “Practical Statistics in Medicine with R”. Over the years, I’ve contributed to the R community by developing three open source R packages in collaborative and independently: “ccaR”, “amstar2Vis”, “musicolor”. Additionally, I maintain the first database that contains a list of a large number of healthcare overviews of reviews (“OMG-database”), along with a website, “RECREATING DATA VISUALIZATIONS IN R”, where I recreate published data visualizations sourced from scholarly articles, websites, or media sources.\n \n Academic publications\nI have contributed to more than 40 peer-reviewed research papers published in high-impact academic journals including: BMJ-British Medical Journal, Research Synthesis Methods, Journal of Clinical Epidemiology, Current Problems in Cardiology, Metabolism,Critical Reviews in Oncology/Hematology, Journal of Dentistry, and PLoS Computational Biology."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Konstantinos I. Bougioukas MSc, PhD",
    "section": "",
    "text": "I am a research methodologist and data analyst specializing in evidence synthesis, an applied statistician, and an independent data visualization enthusiast based in Greece. Driven by a profound interest in meta-research within the medical domain, social sciences, and public health, I am deeply engaged in exploring avenues to enhance the integrity, rigor, and reproducibility of data analysis and visualization. Recently, I have also begun exploring the applications of artificial intelligence in healthcare and evidence synthesis tasks, aiming to optimize decision-making processes and research outcomes.\nI have extensive experience teaching Statistics and leading R practical sessions as an Adjunct Lecturer at the Aristotle University of Thessaloniki. Complementing my teaching role, I authored the free, online version of the textbook “Practical Statistics in Medicine with R”. Over the years, I’ve contributed to the R community by developing three open source R packages in collaborative and independently: “ccaR”, “amstar2Vis”, “musicolor”. Additionally, I maintain the first database that contains a list of a large number of healthcare overviews of reviews (“OMG-database”), along with a website, “RECREATING DATA VISUALIZATIONS IN R”, where I recreate published data visualizations sourced from scholarly articles, websites, or media sources.\n \n Academic publications\nI have contributed to more than 40 peer-reviewed research papers published in high-impact academic journals including: BMJ-British Medical Journal, Research Synthesis Methods, Journal of Clinical Epidemiology, Current Problems in Cardiology, Metabolism,Critical Reviews in Oncology/Hematology, Journal of Dentistry, and PLoS Computational Biology."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Konstantinos I. Bougioukas MSc, PhD",
    "section": "Recent Posts",
    "text": "Recent Posts\nCheck out the latest  Papers ,  Statistics ,  Visualizations ,  News , and  More »\n\n\n\n\n\n\n\n\n\n\nThe Cross-Lagged Panel Model: A starting point for understanding longitudinal SEM\n\n\nThe cross-lagged panel model (CLPM) is a path analysis model that is…\n\n\n\nKonstantinos Bougioukas\n\n\nJan 10, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron: the first artificial neuron\n\n\nThe perceptron is one of the earliest and simplest models of an…\n\n\n\nKonstantinos Bougioukas\n\n\nJan 3, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-sample permutation test: How it works\n\n\nPermutation tests approximate the null distribution of a test statistic…\n\n\n\nKonstantinos Bougioukas\n\n\nDec 15, 2025\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\nAll Posts »"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Konstantinos I. bougioukas\nE-mail: mpougioukas@auth.gr\n\n\n\n\n Back to top"
  },
  {
    "objectID": "links.html#statistics-and-epidemiology",
    "href": "links.html#statistics-and-epidemiology",
    "title": "Useful links",
    "section": "Statistics and Epidemiology",
    "text": "Statistics and Epidemiology\nThe Epidemiologist R Handbook (online handbook)\n\nTutorials in Biostatistics Papers (collection of papers)\n\nBiostatistics for Biomedical Research (on-line handbook)\n\n Doing Meta-Analysis with R: A Hands-On Guide\n(on-line version of the textbook)"
  },
  {
    "objectID": "links.html#data-visualization",
    "href": "links.html#data-visualization",
    "title": "Useful links",
    "section": "Data visualization",
    "text": "Data visualization\nThe R Graph Gallery (website)\n\nThe Science of Visual Data Communication: What Works (paper)\n\n ggplot2: Elegant Graphics for Data Analysis (3e)\n(on-line version of the textbook-3rd edition)\n\n Modern Data Visualization with R\n(on-line version of the textbook)\n\n The Data Visualisation Catalogue (website)\n\n Fundamentals of Data Visualization (on-line version of the textbook)\n\n1 dataset 100 visualizations (website)\n\nTen simple rules for designing graphical abstracts (paper)\n\ncolorspace: A Toolbox for Manipulating and Assessing Colors and Palettes (website)"
  },
  {
    "objectID": "links.html#research-methods-and-open-science",
    "href": "links.html#research-methods-and-open-science",
    "title": "Useful links",
    "section": "Research Methods and Open Science",
    "text": "Research Methods and Open Science\nGlobal movement to reform researcher assessment gains traction (paper)\n\nLIGHTS A living inventory of methods guidance papers (website)\n\nThe Turing Way handbook to reproducible, ethical and collaborative data science (online handbook)\n\nA 24‑step guide on how to design, conduct, and successfully publish a systematic review and meta‑analysis in medical research (paper)\n\nInvestigating and dealing with publication bias and other reporting biases in meta-analyses of health research: A review (paper)\n\nRight Review (website)\n\nHow to keep up to date with medical information using web-based resources: a systematised review and narrative synthesis (paper)\n\nTen simple rules for good research practice (paper)\n\nTen simple rules for implementing open and reproducible research practices after attending a training course (paper)\n\nEvidence Synthesis Hackathon (website)\n\nCochrane Handbook for Systematic Reviews of Interventions (online handbook)\n\nTesting Treatments (available in 15 languages). (online handbook)\n\nJBI Manual for Evidence Synthesis (online manual)\n\nEQUATOR network (website)\n\nLATITUDES network (website)\n\n Declaration on Research Assessment (DORA) (website)\n\n Center for Open Science (COS) (website)\n\nReproducibiliTea (website)\n\nOctopus platform (website)\n\nsearchRxiv Sharing search strategies for better evidence synthesis (website)"
  },
  {
    "objectID": "posts/2024-02-05-amstar2Vis/index.html#abstract",
    "href": "posts/2024-02-05-amstar2Vis/index.html#abstract",
    "title": "amstar2Vis: An R package for presenting the critical appraisal of systematic reviews based on the items of AMSTAR 2",
    "section": "Abstract",
    "text": "Abstract\nSystematic reviews (SRs) have an important role in the healthcare decision-making practice. Assessing the overall confidence in the results of SRs using quality assessment tools, such as “A MeaSurement Tool to Assess Systematic Reviews 2” (AMSTAR 2), is crucial since not all SRs are conducted using the most rigorous methods. In this article, we introduce a free, open-source R package called “amstar2Vis” https://github.com/bougioukas/amstar2Vis that provides easy-to-use functions for presenting the critical appraisal of SRs, based on the items of AMSTAR 2 checklist. An illustrative example is outlined, describing the steps involved in creating a detailed table with the item ratings and the overall confidence ratings, generating a stacked bar plot that shows the distribution of ratings as percentages of SRs for each AMSTAR 2 item, and creating a “ggplot2” graph that shows the distribution of overall confidence ratings (“Critically Low,” “Low,” “Moderate,” or “High”). We expect “amstar2Vis” to be useful for overview authors and methodologists who assess the quality of SRs with AMSTAR 2 checklist and facilitate the production of pertinent publication-ready tables and figures. Future research and applications could further investigate the functionality or potential improvements of our package."
  },
  {
    "objectID": "posts/2024-02-05-amstar2Vis/index.html#links",
    "href": "posts/2024-02-05-amstar2Vis/index.html#links",
    "title": "amstar2Vis: An R package for presenting the critical appraisal of systematic reviews based on the items of AMSTAR 2",
    "section": "Links",
    "text": "Links\nPublished paper"
  },
  {
    "objectID": "posts/2024-02-09-cambridge/index.html",
    "href": "posts/2024-02-09-cambridge/index.html",
    "title": "Our paper is a suggested reading from the University of Cambridge",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "posts/2025-11-30-qqplot/index.html",
    "href": "posts/2025-11-30-qqplot/index.html",
    "title": "Normal Q-Q plot: A step-by-step approach",
    "section": "",
    "text": "A normal Q-Q plot is a visualization for assessing whether a given sample is consistent with a normal (Gaussian) distribution. It is constructed by plotting the theoretical quantiles of a standard normal distribution on the horizontal axis against the sample quantiles (i.e., the ordered values of the dataset) on the vertical axis.\n\nSample quantiles\nLet Y be a random variable with \\(n\\) observations \\(y_1, y_2,..., y_n\\). First, we sort the observations from smallest to largest: \\(y_{(1)} \\leq y_{(2)} \\leq \\cdots \\leq y_{(n)}\\). Then, for each ordered value \\(y_{(i)}\\) (sample quantile), we assign a cumulative probability—referred to as plotting position—according to Hazen’s formula \\(P_i = \\frac{i-0.5}{n}\\), where \\(i = 1, 2, ..., n\\) (Table 1). This convention, originally proposed by Hazen (1914) and later discussed by Cunnane (1978), is often applied for sufficiently large sample sizes (e.g. n &gt; 10).\n\n\n\nTable 1: Sample quantiles and plotting positions for \\(n&gt;10\\)\n\n\n\n\n\n\n\n\n\nSample quantile  \\(y_{(i)}\\)\nPlotting position  \\(P_i = \\frac{i-0.5}{n}\\)\n\n\n\n\n\\(y_{(1)}\\)\n\\(\\frac{0.5}{n}\\)\n\n\n\\(y_{(2)}\\)\n\\(\\frac{1.5}{n}\\)\n\n\n\\(y_{(3)}\\)\n\\(\\frac{2.5}{n}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(y_{(n)}\\)\n\\(\\frac{n-0.5}{n}\\)\n\n\n\n\n\n\n​\nExample: Twenty 9-year-old children with age-appropriate development completed a visual matching task on a computer. A target image appeared on the left, and either an identical copy or mirror image appeared on the right. Children pressed one key for matches and another for mirror images. The data represents reaction times (RT) in milliseconds for the 20 children (Table 2).\n\n\n\nTable 2: Reaction times (RT) in milliseconds for 20 children\n\n\n\n\n\n2549\n1938\n1698\n1725\n1236\n2953\n1367\n837\n1843\n1369\n\n\n1400\n3696\n5179\n1584\n1908\n1415\n4417\n1500\n2265\n1687\n\n\n\n\n\n\n​\nApplying Hazen’s rule, we compute the plotting positions (cumulative probability estimates) for each sample quantile, as shown in Table 3.\n​\n\n\n\nTable 3: Sample quantiles and plotting positions for 20 reaction times\n\n\n\n\n\n\n\n\n\n\n\nSample quantile  \\(y_{(i)}\\)\nPlotting position  \\(P_i = (i-0.5)/20\\)\n\n\n\n\n\n837\n0.025\n\n\n\n1236\n0.075\n\n\n\n1367\n0.125\n\n\n\n1369\n0.175\n\n\n\n1400\n0.225\n\n\n\n1415\n0.275\n\n\n\n1500\n0.325\n\n\n\n1584\n0.375\n\n\n\n1687\n0.425\n\n\n\n1698\n0.475\n\n\n\n1725\n0.525\n\n\n\n1843\n0.575\n\n\n\n1908\n0.625\n\n\n\n1938\n0.675\n\n\n\n2265\n0.725\n\n\n\n2549\n0.775\n\n\n\n2953\n0.825\n\n\n\n3696\n0.875\n\n\n\n4417\n0.925\n\n\n\n5179\n0.975\n\n\n\n\n\n\n​\nIn R\nThe reaction times are:\n\ny &lt;- c(2549, 1938, 1698, 1725, 1236, 2953, 1367, 837, 1843, 1369, \n       1400, 3696, 5179, 1584, 1908, 1415, 4417, 1500, 2265, 1687)\n\n​\nWe sort the these observations form the smallest to largest:\n\ny_sorted &lt;- sort(y)\ny_sorted\n\n [1]  837 1236 1367 1369 1400 1415 1500 1584 1687 1698 1725 1843 1908 1938 2265\n[16] 2549 2953 3696 4417 5179\n\n\n​\nThen, we compute the plotting position \\(P_i\\) for each \\(i = 1, 2, ..., n\\):\n\nn &lt;- length(y)\ni &lt;- seq_len(n)\nPi &lt;- (i - 0.5) / n\nPi\n\n [1] 0.025 0.075 0.125 0.175 0.225 0.275 0.325 0.375 0.425 0.475 0.525 0.575\n[13] 0.625 0.675 0.725 0.775 0.825 0.875 0.925 0.975\n\n\n​\n\n\nTheoretical quantiles\nThe theoretical quantiles are the z-values from the standard normal distribution corresponding to the plotting positions. For each cumulative probability \\(P_i\\), the theoretical quantile \\(z_i\\) is the value such that: \\(P(Z&lt; z_i) = P_i\\), where \\(Z \\sim N(0,1)\\) is a standard normal random variable.\nThe z-value can be calculated using \\(z_i = \\Phi^{-1}(P_i)\\), where \\(\\Phi^{-1}\\) is the inverse cumulative distribution function of the standard normal distribution. As an example, for the first (\\(i=1\\)) plotting position, we have \\(P_1 = 0.025\\). Therefore: \\(P(Z &lt; z_1) = 0.025 \\Rightarrow z_1 = \\Phi^{-1}(0.025) \\approx -1.96\\) (Table 4). This means that 2.5% of the standard normal distribution falls below \\(z_1 = -1.96\\).\n\n\n\nTable 4: Plotting positions and theoretical quantiles\n\n\n\n\n\n\n\n\n\nPlotting position  \\(P_i\\)\nTheoretical quantile  \\(z_i = \\Phi^{-1}(P_i)\\)\n\n\n\n\n0.025\n-1.96\n\n\n0.075\n-1.44\n\n\n0.125\n-1.15\n\n\n0.175\n-0.93\n\n\n0.225\n-0.76\n\n\n0.275\n-0.60\n\n\n0.325\n-0.45\n\n\n0.375\n-0.32\n\n\n0.425\n-0.19\n\n\n0.475\n-0.06\n\n\n0.525\n0.06\n\n\n0.575\n0.19\n\n\n0.625\n0.32\n\n\n0.675\n0.45\n\n\n0.725\n0.60\n\n\n0.775\n0.76\n\n\n0.825\n0.93\n\n\n0.875\n1.15\n\n\n0.925\n1.44\n\n\n0.975\n1.96\n\n\n\n\n\n\nIn R\nThe theoretical quantiles can be calculated using the qnorm() function:\n\nz_i &lt;- qnorm(Pi)\nz_i\n\n [1] -1.95996398 -1.43953147 -1.15034938 -0.93458929 -0.75541503 -0.59776013\n [7] -0.45376219 -0.31863936 -0.18911843 -0.06270678  0.06270678  0.18911843\n[13]  0.31863936  0.45376219  0.59776013  0.75541503  0.93458929  1.15034938\n[19]  1.43953147  1.95996398\n\n\n\n\nThe Q-Q plot\nThe last step is to generate the normal Q-Q plot by plotting the theoretical quantiles (\\(z_i\\)) on the x-axis against the sample quantiles (\\(y_{(i)}\\)) on the y-axis using the values from Table 5.\n\n\n\nTable 5: Theoretical quantiles and sample quantiles for RT data\n\n\n\n\n\n\n\n\n\nTheoretical quantile  \\(z_i\\)\nSample quantile  \\(y_{(i)}\\)\n\n\n\n\n-1.96\n837\n\n\n-1.44\n1236\n\n\n-1.15\n1367\n\n\n-0.93\n1369\n\n\n-0.76\n1400\n\n\n-0.60\n1415\n\n\n-0.45\n1500\n\n\n-0.32\n1584\n\n\n-0.19\n1687\n\n\n-0.06\n1698\n\n\n0.06\n1725\n\n\n0.19\n1843\n\n\n0.32\n1908\n\n\n0.45\n1938\n\n\n0.60\n2265\n\n\n0.76\n2549\n\n\n0.93\n2953\n\n\n1.15\n3696\n\n\n1.44\n4417\n\n\n1.96\n5179\n\n\n\n\n\n\n\n# Make QQ plot\nplot(z_i, y_sorted,\n     xlab = \"Theoretical Quantiles\", \n     ylab = \"Sample Quantiles\", \n     main = \"Normal Q-Q Plot\",\n     pch = 1, col = \"black\")\n\n\n\n\n\n\n\n\n\n\nAdding a reference line (Quartile Method)\nThe Quartile Method is a robust technique for defining a reference line on a Quantile-Quantile (Q-Q) plot. The line passes through the first quartile (\\(Q_1\\)) and third quartile (\\(Q_3\\)) of both the theoretical and sample distributions.\nThe reference line is \\(y = b_o + b_1 z\\), where the slope and intercept are calculated as:\n\\(b_1 = \\frac{y_{0.75} - y_{0.25}}{z_{0.75} - z_{0.25}}\\)\n\\(b_0 = y_{0.25} - b_1 \\, z_{0.25}\\)\nHere, \\(z_{0.25}\\) and \\(z_{0.75}\\) are the first and third quartiles of the theoretical standard normal distribution, while \\(y_{0.25}\\) and \\(y_{0.75}\\) are the corresponding quartiles of the sample.\n​\nIn R\n\n# Compute quartiles\nz25 &lt;- qnorm(0.25)\nz75 &lt;- qnorm(0.75)\ny25 &lt;- quantile(y, 0.25)\ny75&lt;- quantile(y, 0.75)\n\n# Compute line parameters\nb1 &lt;- (y75 - y25) / (z75 - z25)\nb0 &lt;- y25 - b1 * z25\n\n​\n\n# Make QQ plot\nplot(z_i, y_sorted,\n     xlab = \"Theoretical Quantiles\", \n     ylab = \"Sample Quantiles\", \n     main = \"Normal Q-Q Plot\",\n     pch = 1, col = \"black\")\n\n# Add the quartile-based line\nabline(b0, b1, col = \"red\")\n\n\n\n\n\n\n\n\nThe Q–Q plot shows clear deviations from the reference line, particularly in the upper tail, indicating that the data are not normally distributed and exhibit positive skewness. This conclusion is further supported by the histogram and boxplot, which display an asymmetric distribution of reaction times, with a long right tail and two outliers.\n\nlayout(mat = matrix(c(1,2), ncol = 1), heights = c(2, 8))\n\n# Top: horizontal boxplot\npar(mar = c(0, 4, 1, 2))\nboxplot(y, horizontal = TRUE, axes = FALSE, \n        col = \"lightgray\", outline = TRUE)\n\n# Bottom: histogram\npar(mar = c(5, 4, 0, 2))\nhist(y, main = \"\")\n\n\n\n\n\n\n\n\n​\nThe normal Q–Q plot can be generated using the built-in R functions qqnorm() and qqline().\n\nqqnorm(y)\nqqline(y, col = \"red\")\n\n\n\n\n\n\n\n\nWe can also add a box‑plot for the sample quantiles to the right-side of the Q–Q plot as follows:\n\n# Set up the plotting layout: 2 columns, first column wider\nlayout(matrix(c(1, 2), nrow = 1), widths = c(3, 0.6))\n\n# Set margins for both plots\npar(mar = c(5, 4, 4, 1))\n\n# Create Q-Q plot\nqqnorm(y)\nqqline(y, col = \"red\")\n\n# Set margins for boxplot (reduce left margin)\npar(mar = c(5, 1, 4, 2))\n\n# Create boxplot on the right\nboxplot(y, yaxt = \"n\", frame = FALSE, ylim = range(y))\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\nCunnane, C. 1978. “Unbiased Plotting Positions — a Review.” Journal of Hydrology 37 (3): 205–22. https://doi.org/10.1016/0022-1694(78)90017-3.\n\n\nHazen, A. 1914. “Storage to Be Provided in Impounding Reservoirs for Municipal Water Supply.” Trans. Amer. Soc. Civ. Eng. Pap 1308 (77): 1547–50.\n\n\n\n\n\n     \n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2025-15-12-permtest/index.html",
    "href": "posts/2025-15-12-permtest/index.html",
    "title": "Two-sample permutation test: How it works",
    "section": "",
    "text": "​"
  },
  {
    "objectID": "posts/2025-15-12-permtest/index.html#permutation-tests",
    "href": "posts/2025-15-12-permtest/index.html#permutation-tests",
    "title": "Two-sample permutation test: How it works",
    "section": "Permutation Tests",
    "text": "Permutation Tests\nPermutation tests, also known as randomization or re-randomization tests, approximate the null distribution of a test statistic by resampling in a manner consistent with the null hypothesis, a concept originating in Fisher’s early work on randomized experiments. They are especially useful when parametric assumptions, such as normality, are questionable or when sample sizes are small.\n\nTwo-sample problem\nThe two-sample permutation test uses this resampling approach to assess whether the locations of two independent groups differ, providing a non-parametric alternative to traditional tests of mean or median differences.\nThis is achieved by repeatedly permuting the data labels without replacement—so that each permutation is a rearrangement of the original labels—and recalculating the test statistic for each permutation. The resulting p-value is the proportion of permutations that produce a statistic as extreme as—or more extreme than—the observed value.\nWe now describe this framework in detail, step by step.\nLet \\(\\mathbf{X_1} = \\{X_{1i}, i = 1, 2, ..., n_1  \\}\\) and \\(\\mathbf{X_2} = \\{X_{2i}, i = 1, 2, ..., n_2  \\}\\) be two sets of independent sample data. The objective is to test whether the data are consistent with the null hypothesis of no difference in locations of the distributions (\\(\\delta = 0\\), where \\(\\delta\\) is known as the treatment effect)\n\\[H_o: \\mathbf{X_1} \\stackrel{d}{=} \\mathbf{X_2}\\]\nor with the two-sided alternative (\\(\\delta \\neq 0\\))\n\\[H_1: \\mathbf{X_1} \\stackrel{d}{\\neq} \\mathbf{X_2}\\]\nStep 1: Combine the two datasets\nFirst, we combine the two datasets in one vector as follows:\n\\[\\mathbf{X} = \\mathbf{X}_1 \\uplus \\mathbf{X}_2 = \\{X_i, i = 1, \\ldots, n\\}, \\ \\ \\ n= n_1 + n_2\\]\nwhere \\(\\uplus\\) denotes concatenation, so that the two samples are pooled into a single set \\(\\mathbf{X}\\), which is fixed.\n​\nStep 2: Create an assignment indicator vector\nThe assignment vector \\(\\mathbf{Z}\\) indicates which group each participant belongs to:\n\\[\\mathbf{Z} = \\{{Z_i, i = 1, \\ldots, n}\\}\\]\nwhere:\n\\[\nZ_i =\n\\begin{cases}\n1, & \\text{if participant } i \\text{ is assigned to } \\mathbf{X}_1,\\\\\n0, & \\text{if participant } i \\text{ is assigned to } \\mathbf{X}_2.\n\\end{cases}\n\\]\nProperties of \\(\\mathbf{Z}\\):\n\nEach \\(Z_i\\) is binary: \\(Z_i \\in \\{0, 1\\}\\)\nThe sum \\(\\sum_{i=1}^{n} Z_i = n_1\\) (counts how many are in Group 1)\nThe sum \\(\\sum_{i=1}^{n} (1 - Z_i) = n_2\\) (counts how many are in Group 2)\n\n​\nTherefore, the observed assignment indicator is:\n\\[\n\\mathbf{Z^{\\text{obs}}} = (\\underbrace{1, 1, \\ldots, 1}_{n_1 \\text{ times}}, \\underbrace{0, 0, \\ldots, 0}_{n_2 \\text{ times}})\n\\]\n​\nStep 3: Define the test statistic (e.g., difference in means)\nThe test statistic is a function of both \\(\\mathbf{Z}\\) and \\(\\mathbf{X}\\):\n\\[T (\\mathbf{Z}, \\mathbf{X}) = \\mathbf{\\bar{X}_1} - \\mathbf{\\bar{X}_2}\\]\nwhere the group means are\n\\[\\mathbf{\\bar{X}_1} = \\frac{1}{n_1} \\sum_{i=1}^{n} Z_i X_i = \\frac{1}{n_1} \\sum_{i: Z_i = 1} X_i\\]\n\\[\\mathbf{\\bar{X}_2} = \\frac{1}{n_2} \\sum_{i=1}^{n} (1 - Z_i) X_i = \\frac{1}{n_2} \\sum_{i: Z_i = 0} X_i\\]\nTherefore, the observed test statistic is:\n\\[T^{obs}=T(\\mathbf{Z^{obs}},\\mathbf{X})\\]\n​\nStep 4: Generate the permutation distribution\nUnder the null hypothesis \\(H_o\\), any permutation of \\(\\mathbf{Z}\\) is just as likely to have produced the observed data \\(\\mathbf{X}\\) as the actual assignment \\(\\mathbf{Z^{obs}}\\). Relying on this assumption, we randomly permute (suffle) the group labels while keeping the data values fixed. For each permutation, we recalculate the test statistic \\(T\\).\nIf we consider all possible orderings of n observations:\n\\[n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1\\]\nthen, for example, if \\(n=16\\), the total number of orderings (i.e., permutations of all 16 items) is enormous:\n\\[16! \\approx 2.09 \\times 10^{13}\\]\nHowever, for a test statistic based on group means, we are only interested in which observations belong to which group—not the order of observations within each group. The mean of a set of numbers remains the same no matter the order in which the numbers are added.\nIn this case, we only need to determine which observations go into the first group; the remaining observations automatically belong to the second group. The number of such distinct assignments (cardinality) is :\n\\[M = \\ ^nC_{n_1} = \\binom{n}{n_1} = \\frac{n!}{n_1! \\, (n - n_1)!}\\] This represents choosing \\(n_1\\) observations out of \\(n\\) to be in Group 1. The remaining \\(n_2 = n - n_1\\) observations then automatically form Group 2.\n\n\n\n\n\n\nA simple example\n\n\n\nLet’s consider a total of \\(n=4\\) observations that are to be split into two groups: Group A with \\(n_1=2\\) observations and Group B with \\(n_2=2\\) observations. The pooled data set, \\(\\mathbf{X}\\), is fixed as the set of values \\(\\{a_1, a_2, b_1, b_2\\}\\).\nAssuming the specific numerical observations are \\(\\{1, 2, 3, 4\\}\\), the possible permutations for splitting this data set into two groups of size two are:\n\n\\(A = \\{1, 2\\} | B = \\{3, 4\\}\\)\n\\(A = \\{1, 3\\} | B = \\{2, 4\\}\\)\n\\(A = \\{1, 4\\} | B = \\{2, 3\\}\\)\n\\(A = \\{2, 3\\} | B = \\{1, 4\\}\\)\n\\(A = \\{2, 4\\} | B = \\{1, 3\\}\\)\n\\(A = \\{3, 4\\} | B = \\{1, 2\\}\\)\n\n\\[M = \\ ^4C_2 = \\binom{4}{2} = \\frac{4!}{2!(4-2)!} = \\frac{4 \\times 3 \\times 2 \\times 1}{(2 \\times 1)(2 \\times 1)} = \\frac{24}{4} = 6\\]\n\n\n​\nThe set of all possible assignment vectors:\n\\[\\Omega = \\Big\\{ \\mathbf{Z}^{(m)} : \\sum_{i=1}^{n} Z_i^{(m)} = n_1, \\ Z_i^{(m)} \\in \\{0,1\\}, \\ m = 1, \\dots, M \\Big\\}\\] For each \\(\\mathbf{Z}^{(m)} \\in \\Omega\\), we compute:\n\\[T^{(m)} = T(\\mathbf{Z}^{(m)}, \\mathbf{X})\\]\nThis gives us the exact permutation distribution:\n\\[\\mathcal{T} = \\{ T^{(1)}, T^{(2)}, \\dots, T^{(M)} \\}\\]\n​\nStep 5: Calculate the p-value\nFor a two-sided exact p-value:\n\\[\np\n=\n\\begin{cases}\n\\displaystyle\n\\frac{1}{M}\n\\sum_{m=1}^{M}\n\\Big[\n\\mathbf{I}\\!\\left(T^{(m)} \\ge T^{\\text{obs}}\\right)\n+\n\\mathbf{I}\\!\\left(T^{(m)} \\le -T^{\\text{obs}}\\right)\n\\Big],\n& if \\ \\ T^{\\text{obs}} &gt; 0,\n\\\\[1.2em]\n\\displaystyle\n\\frac{1}{M}\n\\sum_{m=1}^{M}\n\\Big[\n\\mathbf{I}\\!\\left(T^{(m)} \\le T^{\\text{obs}}\\right)\n+\n\\mathbf{I}\\!\\left(T^{(m)} \\ge -T^{\\text{obs}}\\right)\n\\Big],\n& if \\ \\ T^{\\text{obs}} &lt; 0.\n\\end{cases}\n\\]\nor more compactly\n\\[\np\n=\n\\frac{1}{M}\n\\sum_{m=1}^{M}\n\\mathbf{I}\\!\\left(\n\\left|T^{(m)}\\right|\n\\ge\n\\left|T^{\\text{obs}}\\right|\n\\right)\n\\]\nwhere the indicator function \\(\\mathbf{I}(\\text{condition}) =\n\\begin{cases}\n1, & \\text{if condition is true},\\\\\n0, & \\text{otherwise}.\n\\end{cases}\\)\n\n\nMonte Carlo simulation\nWhen \\(M\\) is large (computationally unfeasible to enumerate all permutations), we use Monte Carlo simulation. In this case, we generate random permutation \\(Z^{(b)}\\) and compute the test statistic on the permuted labels \\(T^{(b)} = T(\\mathbf{Z}^{(b)}, \\mathbf{X}), \\ where \\ b = 1, \\dots, B\\) and \\(B\\) is the number of Monte Carlo permutations. This creates the approximate permutation distribution \\(\\mathcal{T_{aprox}} = \\{T^{(1)}, T^{(2)}, \\ldots, T^{(B)}\\}\\) and the p-value is calculated as:\n\\[\n\\hat{p}_{\\text{MC}}\n=\n\\frac{1}{B}\n\\sum_{b=1}^{B}\n\\mathbf{I}\\!\\left(\n\\left|T^{(b)}\\right|\n\\ge\n\\left|T^{\\text{obs}}\\right|\n\\right)\n\\]\n​"
  },
  {
    "objectID": "posts/2025-15-12-permtest/index.html#numerical-example-application-of-the-framework-in-r",
    "href": "posts/2025-15-12-permtest/index.html#numerical-example-application-of-the-framework-in-r",
    "title": "Two-sample permutation test: How it works",
    "section": "Numerical example: application of the framework in R",
    "text": "Numerical example: application of the framework in R\nLet the sample data\n\\[\\mathbf{X}_1 = \\{66, 57, 81, 62, 61, 60, 73, 53\\} \\quad (n_1=8)\\]\n\\[\\mathbf{X}_2 = \\{64, 58, 59, 44, 47, 56, 48, 51\\} \\quad (n_2=8)\\]\n\n# Data\nX1 &lt;- c(66, 57, 81, 62, 61, 60, 73, 53)   # (n1=8)\nX2 &lt;- c(64, 58, 59, 44, 47, 56, 48, 51)   # (n2=8)\n\n​\nStep 1: Combine the two datasets\nFollowing the permutation test procedure, the two samples are pooled into a single vector \\(\\mathbf{X}\\) of length \\(n=16\\):\\[\\mathbf{X} = \\mathbf{X}_1 \\uplus \\mathbf{X}_2 = \\{66, 57, 81, 62, 61, 60, 73, 53, 64, 58, 59, 44, 47, 56, 48, 51\\}\\]\n\nn1 &lt;- length(X1)\nn2 &lt;- length(X2)\nn &lt;- n1 + n2\nX &lt;- c(X1, X2)\nX\n\n [1] 66 57 81 62 61 60 73 53 64 58 59 44 47 56 48 51\n\n\n​\nStep 2: Create the observed assignment vector\nThe observed assignment vector is:\n\\[\n\\mathbf{Z^{\\text{obs}}} = (\\underbrace{1, 1, 1, 1, 1, 1, 1, 1}_{8 \\text{ times}}, \\underbrace{0, 0, 0, 0, 0, 0, 0, 0}_{8 \\text{ times}})\n\\]\nTherefore:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathbf{X}\\)\n66\n57\n81\n62\n61\n60\n73\n53\n64\n58\n59\n44\n47\n56\n48\n51\n\n\n\\(\\mathbf{Z^{\\text{obs}}}\\)\n1\n1\n1\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n​\nPermutation \\(m=1\\) (Original Assignment)\n\n# Indices for Group 1 (X1) in the pooled vector X\nperm1 &lt;- c(1, 2, 3, 4, 5, 6, 7, 8)\nperm1 \n\n[1] 1 2 3 4 5 6 7 8\n\n\n\n# Initialize the assignment vector Z to all zeros\nZ_1 &lt;- numeric(16)\nZ_1\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\n\n# Assign 1s to the indices corresponding to Group 1\nZ_1[perm1] &lt;- 1    # Put 1s at positions 1-8\nZ_1\n\n [1] 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n\n\n\nZ_obs &lt;- Z_1\nZ_obs\n\n [1] 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n\n\n​\nStep 3: Calculate the observed test statistic\nThe \\(T^{obs}\\) statistic is:\n\\[T^{obs} = T(\\mathbf{Z}^{(obs)}, \\mathbf{X})\\]\n\nX1_mean &lt;- mean(X1)\nX2_mean &lt;- mean(X2)\nT_obs &lt;- X1_mean - X2_mean\nT_obs\n\n[1] 10.75\n\n\n​\nStep 4: Generate the permutation distribution\nNext, the following two permutations are presented (i.e., m = 2 and m = 3).\n\nFor permutation \\(m=2\\) (Different Assignment; the last element is changed to 9)\n\n\n# Indices for Group 1 (X1) in the pooled vector X\nperm2 &lt;- c(1, 2, 3, 4, 5, 6, 7, 9)     # now last element is 9\nperm2 \n\n[1] 1 2 3 4 5 6 7 9\n\n\n\n# Initialize the assignment vector Z to all zeros\nZ_2 &lt;- numeric(16)\nZ_2\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\n\n# Assign 1s to the indices corresponding to Group 1\nZ_2[perm2] &lt;- 1    # Put 1s at positions 1-7 and 9\nZ_2\n\n [1] 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n\n\n\nX1_perm2 &lt;- X[Z_2 == 1]\nX1_perm2\n\n[1] 66 57 81 62 61 60 73 64\n\nX2_perm2 &lt;- X[Z_2 == 0]\nX2_perm2\n\n[1] 53 58 59 44 47 56 48 51\n\n\nThe T statistic is:\n\\[T^{(2)} = T(\\mathbf{Z}^{(2)}, \\mathbf{X})\\]\n\nT_perm2 &lt;- mean(X1_perm2) - mean(X2_perm2)\nT_perm2\n\n[1] 13.5\n\n\n​\n\nSimilarly, for permutation \\(m=3\\) (Different Assignment; the last element is changed to 10)\n\n\n# Indices for Group 1 (X1) in the pooled vector X\nperm3 &lt;- c(1, 2, 3, 4, 5, 6, 7, 10)    # now the last element is 10\nperm3 \n\n[1]  1  2  3  4  5  6  7 10\n\n\n\n# Initialize the assignment vector Z to all zeros\nZ_3 &lt;- numeric(16)\nZ_3\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\n\n# Assign 1s to the indices corresponding to Group 1\nZ_3[perm3] &lt;- 1    # Put 1s at positions 1-7 and 10\nZ_3\n\n [1] 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0\n\n\n\nX1_perm3 &lt;- X[Z_3 == 1]\nX1_perm3\n\n[1] 66 57 81 62 61 60 73 58\n\nX2_perm3 &lt;- X[Z_3 == 0]\nX2_perm3\n\n[1] 53 64 59 44 47 56 48 51\n\n\nThe T statistic is:\n\\[T^{(3)} = T(\\mathbf{Z}^{(3)}, \\mathbf{X})\\]\n\nT_perm3 &lt;- mean(X1_perm3) - mean(X2_perm3)\nT_perm3\n\n[1] 12\n\n\n​\nNow, the total number of possible permutations is computed as:\n\nM &lt;- choose(n, n1)\nM\n\n[1] 12870\n\n\nIn this case, it is feasible to generate all possible permutations:\n\n# generate  all possible combinations of choosing n1 elements from the set X\nlibrary(combinat)\nall_perms &lt;- combn(1:n, n1)\n\nThe first three permutations, as found previously, are:\n\nall_perms[, c(1:3)]\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    2    2    2\n[3,]    3    3    3\n[4,]    4    4    4\n[5,]    5    5    5\n[6,]    6    6    6\n[7,]    7    7    7\n[8,]    8    9   10\n\n\n​\nThe collection of all possible T statistics—one for each possible way to divide the \\(n\\) observations into groups of size \\(n_1\\), \\(\\mathcal{T} = \\{ T^{(1)}, T^{(2)}, \\dots, T^{(12870)} \\}\\), can be generated using the following loop:\n\nT_perm &lt;- numeric(M)\n\nfor (m in 1:M) {\n  # Create assignment vector for this permutation\n  Z_m &lt;- numeric(n)\n  Z_m[all_perms[, m]] &lt;- 1\n  \n  # Calculate test statistic for this permutation\n  X1_perm &lt;- X[Z_m == 1]\n  X2_perm &lt;- X[Z_m == 0]\n  T_perm[m] &lt;- mean(X1_perm) - mean(X2_perm)\n}\n\nWe can print the first three T static values as follows (we have computed them previously):\n\nT_perm[1:3]\n\n[1] 10.75 13.50 12.00\n\n\n​\nFinally, we create the histogram of the computed T statistics that is the exact permutation distribution. Each bar represents the number of times a particular range of \\(T\\) values occurs across all possible permutations of the data.\n\nhist(T_perm, breaks = 50, col = rgb(0.8, 1, 0.8), \n     main = \"Exact Permutation Distribution\",\n     xlab = \"Test Statistic T\", xlim = c(-15, 15))\n\n\n\n\n\n\n\n\nStep 5: Calculate the p-value\nSince the \\(T^{obs} = 10.75 &gt; 0\\):\n\np_value_exact &lt;- (sum(T_perm &gt;= T_obs) + sum(T_perm &lt;= -T_obs)) / M\np_value_exact\n\n[1] 0.01787102\n\n\n​\nThe extreme values of T statistic, which are unlikely under the null hypothesis, are highlighted in a light red color. The upper tail corresponds to \\(T \\ge T^{\\text{obs}}\\), and the lower tail corresponds to \\(T \\le -T^{\\text{obs}}\\).\n\n# Histogram of permutation distribution with colored tails\nh &lt;- hist(T_perm, breaks = 50, plot = FALSE)\ncolors &lt;- ifelse(h$mids &gt;= T_obs | h$mids &lt;= -T_obs, \n                 rgb(1, 0, 0, 0.5), rgb(0.8, 1, 0.8))\n\nplot(h, col = colors, main = \"Exact Permutation Distribution\",\n     xlab = \"Test Statistic T\", xlim = c(-15, 15))\nabline(v = T_obs, col = \"darkred\", lwd = 2, lty = 2)\ntext(T_obs, max(h$counts) * 0.95,\n     bquote(T^{obs} == .(round(T_obs, 2))),\n     col = \"darkred\", cex = 1, font = 2, adj = c(0.5, 0))\n\n\n\n\n\n\n\n\nor equivalently\n\np_value_exact &lt;- mean(abs(T_perm) &gt;= abs(T_obs))\np_value_exact\n\n[1] 0.01787102\n\n\nThe corresponding p-value is \\(p = 0.0179\\). Since \\(p &lt; 0.05\\), the observed test statistic lies in the extreme tails of the permutation distribution, indicating that the observed grouping is unusual under the null hypothesis. Therefore, we reject the null hypothesis at the 5% significance level, suggesting a statistically significant difference between the groups.\n​\nThe result can also be confirmed by using the oneway_test() function from the coin R package:\n\ncoin::oneway_test(X~factor(Z_obs), distribution=\"exact\")\n\n\n    Exact Two-Sample Fisher-Pitman Permutation Test\n\ndata:  X by factor(Z_obs) (0, 1)\nZ = -2.2489, p-value = 0.01787\nalternative hypothesis: true mu is not equal to 0\n\n\n​"
  },
  {
    "objectID": "posts/2025-15-12-permtest/index.html#comparison-to-the-classic-two-sample-t-test",
    "href": "posts/2025-15-12-permtest/index.html#comparison-to-the-classic-two-sample-t-test",
    "title": "Two-sample permutation test: How it works",
    "section": "Comparison to the classic two-sample T-test",
    "text": "Comparison to the classic two-sample T-test\nLet’s formally apply the two-sample t-test to compare its p-value with that of the permutation test.\n\nt.test(X1, X2)\n\n\n    Welch Two Sample t-test\n\ndata:  X1 and X2\nt = 2.6686, df = 13.108, p-value = 0.0192\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  2.054571 19.445429\nsample estimates:\nmean of x mean of y \n   64.125    53.375 \n\n\nIn this example, the results are highly consistent. Both the exact Permutation Test (\\(p = 0.0179\\)) and the two-sample T-test (\\(p = 0.0192\\)) yield close p-values."
  },
  {
    "objectID": "publications.html#total-citations-as-of-january-2025",
    "href": "publications.html#total-citations-as-of-january-2025",
    "title": "Publications and metrics",
    "section": "Total citations as of January 2025",
    "text": "Total citations as of January 2025\n\n\n\n\nGoogle Scholar: 2052 (h-index = 18)\nScopus: 1431 (h-index = 15)\nWeb of Science: 1351 (h-index = 14)\nResearch Gate: 1738 (h-index = 17)"
  },
  {
    "objectID": "publications.html#peer-reviewed-journal-papers",
    "href": "publications.html#peer-reviewed-journal-papers",
    "title": "Publications and metrics",
    "section": "Peer-reviewed Journal Papers",
    "text": "Peer-reviewed Journal Papers\nBougioukas KI, Liakos A, Tsapas A, Ntzani E, Haidich A-B. Preferred reporting items for overviews of systematic reviews including harms checklist: a pilot tool to be used for balanced reporting of benefits and harms. Journal of Clinical Epidemiology 2018;93:9–24. doi: 10.1016/j.jclinepi.2017.10.002.\n\n\n\n\n\n\n\nAvgerinos KI, Spyrou N, Bougioukas KI, Kapogiannis D. Effects of creatine supplementation on cognitive function of healthy individuals: A systematic review of randomized controlled trials. Experimental Gerontology 2018;108:166–73. doi: 10.1016/j.exger.2018.04.013.\n\n\n\n\n\n\n\nBougioukas KI, Bouras E, Apostolidou-Kiouti F, Kokkali S, Arvanitidou M, Haidich A-B. Reporting guidelines on how to write a complete and transparent abstract for overviews of systematic reviews of health care interventions. Journal of Clinical Epidemiology 2019;106:70–9. doi: 10.1016/j.jclinepi.2018.10.005.\n\n\n\n\n\n\n\nBouras E, Karakioulaki M, Bougioukas KI, Aivaliotis M, Tzimagiorgis G, Chourdakis M. Gene promoter methylation and cancer: An umbrella review. Gene 2019;710:333–40. doi: 10.1016/j.gene.2019.06.023.\n\n\n\n\n\n\n\nDelli FS, Sotiriou E, Lazaridou E, Apalla Z, Lallas A, Vakirlis E, Gerou S, Bougioukas KI, Ioannides D. Total IgE, eosinophils, and interleukins 16, 17A, and 23 correlations in severe bullous pemphigoid and treatment implications. Dermatologic Therapy 2020;33:e13958. doi: 10.1111/dth.13958.\n\n\n\n\n\n\n\nBougioukas KI, Bouras EC, Avgerinos KI, Dardavessis T, Haidich A-B. How to keep up to date with medical information using web-based resources: a systematised review and narrative synthesis. Health Information & Libraries Journal 2020;37:254–92. doi: 10.1111/hir.12318.\n\n\n\n\n\n\n\nBougioukas KI, Vounzoulaki E, Mantsiou CD, Savvides ED, Karakosta C, Diakonidis T, Tsapas A, Haidich A-B. Methods for depicting overlap in overviews of systematic reviews: An introduction to static tabular and graphical displays. Journal of Clinical Epidemiology 2021;132:34–45. doi: 10.1016/j.jclinepi.2020.12.004.\n\n\n\n\n\n\n\nKarakosta C, Bougioukas KI, Karra M, Kontopanos G, Methenitis G, Liaskou M, Paraskevopoulos K, Kokolaki A. Changes in astigmatism after horizontal muscle recession strabismus surgery: A retrospective cohort study. Indian Journal of Ophthalmology 2021;69:1888–93. doi: 10.4103/ijo.IJO_3228_20\n\n\n\n\n\n\n\nBougioukas KI, Vounzoulaki E, Mantsiou CD, Papanastasiou GD, Savvides ED, Ntzani EE, Haidich A-B. Global mapping of overviews of systematic reviews in healthcare published between 2000 and 2020: a bibliometric analysis. Journal of Clinical Epidemiology 2021;137:58–72. doi: 10.1016/j.jclinepi.2021.03.019\n\n\n\n\n\n\n\nGates M, Gates A, Pieper D, Fernandes RM, Tricco AC, Moher D, Brennan S E, Li T, Pollock M, Lunny C, Sepúlveda D, McKenzie JE, Scott SD, Robinson KA, Matthias K, Bougioukas KI, Fusar-Poli P., Whiting P, Moss SJ, & Hartling L. Reporting guideline for overviews of reviews of healthcare interventions: development of the PRIOR statement. BMJ, 378, 2022:e070849. doi: 10.1136/bmj-2022-070849\n\n\n\n\n\n\n\nKarassa FB, Bougioukas KI, Pelechas E, Skalkou A, Argyriou E, Haidich A-B. Pharmacological treatment for connective tissue disease-associated interstitial lung involvement: Protocol for an overview of systematic reviews and meta-analyses. PLOS ONE 2022;17:e0272327. doi: 10.1371/journal.pone.0272327.\n\n\n\n\n\n\n\nGeorgoulis V, Haidich A-B, Bougioukas KI, Hatzimichael E. Efficacy and safety of carfilzomib for the treatment of multiple myeloma: an overview of systematic reviews. Critical Reviews in Oncology/Hematology 180, 2022:103842. doi: 10.1016/j.critrevonc.2022.103842\n\n\n\n\n\n\n\nKarakosta C, Paraskevopoulos K, Bisoukis A, Bougioukas KΙ, Kokolaki A. Straatsma Syndrome: A Case Series. Cureus 2022;14(9): e29779. doi: 10.7759/cureus.29779.\n\n\n\n\n\n\n\nPagkalidou E, Doundoulakis I, Apostolidou-Kiouti F, Bougioukas KI, Papadopoulos K, Tsapas A, et al. An overview of systematic reviews on imaging tests for diagnosis of pulmonary embolism applying different network meta-analytic methods. Hellenic Journal of Cardiology 2023. doi: 10.1016/j.hjc.2023.05.006\n\n\n\n\n\n\n\nPamporis K, Karakasis P, Bougioukas KI, Simantiris S, Sagris M, Bougioukas KI, Tousoulis D. Effectiveness and safety of injectable PCSK9 inhibitors in dyslipidaemias’ treatment and cardiovascular disease prevention: an overview of 86 systematic reviews and a network meta-analysis. Clínica e Investigación en Arteriosclerosis 2023. doi: 10.1016/j.arteri.2023.11.003\n\n\n\n\n\n\n\nKarakasis P, Patoulias D, Pamporis K, Stachteas P, Lefkou E, Bougioukas KI, et al. Risk of subclinical atherosclerosis in primary Sjogren’s syndrome: A systematic review and meta-analysis. European Journal of Internal Medicine 2023;0. doi: 10.1016/j.ejim.2023.11.007.\n\n\n\n\n\n\n\nKarakasis P, Bougioukas KI, Pamporis K, Fragakis N, Anna-Bettina Haidich A-B. Appraisal methods and outcomes of AMSTAR 2 assessments in overviews of systematic reviews of interventions in the cardiovascular field: A methodological study. Research Synthesis Methods. 2023. doi: 10.1002/jrsm.1680\n\n\n\n\n\n\n\nMainou M, Bougioukas KI, Malandris K, Liakos A, Klonizakis P, Avgerinos I, et al. Reporting of adverse events of treatment interventions in multiple myeloma: an overview of systematic reviews. Ann Hematol 2023. doi: 10.1007/s00277-023-05517-7.\n\n\n\n\n\n\n\nHeise V, Holman C, Lo H, Lyras EM, Adkins MC, Maria Raisa Jessica Aquino, Bougioukas KI, O. Bray K., et al. Ten simple rules for implementing open and reproducible research practices after attending a training course. Plos Computational Biology. 2023;19:e1010750 doi: 10.1371/journal.pcbi.1010750.\n\n\n\n\n\n\n\nBougioukas KI, Pamporis K, Vounzoulaki E, Karagiannis T, Haidich A-B. Types and associated methodologies of overviews of reviews in healthcare: a methodological study with published examples. Journal of Clinical Epidemiology 2023;153:13–25. doi: 10.1016/j.jclinepi.2022.11.003.\n\n\n\n\n\n\n\nBougioukas KI, Diakonidis T, Mavromanoli AC, Haidich A-B. ccaR: a package for assessing primary study overlap across systematic reviews in overviews. Research Synthesis Methods. 2023; 14(3):443–454. doi: 10.1002/jrsm.1610.\n\n\n\n\n\n\n\nChatzidimitriou K, Papaioannou W, Seremidi K, Bougioukas K, Haidich A-B. Prevalence and association of gastroesophageal reflux and dental erosion: An overview of reviews. Journal of Dentistry 133, 2023:104520. doi: 10.1016/j.jdent.2023.104520.\n\n\n\n\n\n\n\nKarakasis P, Lefkou E, Pamporis K, Nevras V, Bougioukas KI, Haidich A-B, et al. Risk of subclinical atherosclerosis in patients with antiphospholipid syndrome and subjects with antiphospholipid antibody positivity: A systematic review and meta-analysis. Current Problems in Cardiology 2023; 48(6):101672. doi: 10.1016/j.cpcardiol.2023.101672.\n\n\n\n\n\n\n\nPamporis K, Bougioukas KI, Karakasis P, Papageorgiou D, Zarifis I., Haidich A-B. Overviews of reviews in the cardiovascular field underreported critical methodological and transparency characteristics: a methodological study based on the PRIOR statement. Journal of Clinical Epidemiology 2023;159:139–150. doi: 10.1016/j.jclinepi.2023.05.018.\n\n\n\n\n\n\n\nKarakasis P., Pamporis K., Stachteas P. Patoulias D, Bougioukas KI, Nikolaos Fragakis N. Efficacy and safety of sodium-glucose cotransporter-2 inhibitors in heart failure with mildly reduced or preserved ejection fraction: an overview of 36 systematic reviews. Heart Fail Rev 2023;28(5):1033–1051. doi: 10.1007/s10741-023-10324-3.\n\n\n\n\n\n\n\nFouza A, Tagkouta A, Daoudaki M, Stangou M, Fylaktou A, Bougioukas K, et al. Exploring Perturbations in Peripheral B Cell Memory Subpopulations Early after Kidney Transplantation Using Unsupervised Machine Learning. Journal of Clinical Medicine 2023;12(19):6331. doi: 10.3390/jcm12196331.\n\n\n\n\n\n\n\nKarakasis P, Patoulias D, Pamporis K, Stachteas P, Bougioukas KI, Klisic A, et al. Safety and efficacy of the new, oral, small-molecule, GLP-1 receptor agonists orforglipron and danuglipron for the treatment of type 2 diabetes and obesity: systematic review and meta-analysis of randomized controlled trials. Metabolism 2023;149:155710. doi: 10.1016/j.metabol.2023.155710.\n\n\n\n\n\n\n\nKarakasis P, Patoulias D, Pamporis K, Popovic DS, Stachteas P, Bougioukas KI, et al. Efficacy and safety of once-weekly versus once-daily basal insulin analogues in the treatment of type 2 diabetes mellitus: A systematic review and meta-analysis. Diabetes, Obesity and Metabolism 2023; 25(12): 3648–3661. doi: 10.1111/dom.15259.\n\n\n\n\n\n\n\nKarakasis P, Stalikas N, Patoulias D, Pamporis K, Karagiannidis E, Sagris M, Stachteas P, Bougioukas KI et al. Prognostic value of stress hyperglycemia ratio in patients with acute myocardial infarction: a systematic review with Bayesian and frequentist meta-analysis. Trends in Cardiovascular Medicine 2024. doi: 10.1016/j.tcm.2023.11.006.\n\n\n\n\n\n\n\nTalimtzi P, Ntolkeras A, Kostopoulos G, Bougioukas KI, Pagkalidou E, Ouranidis A, et al. The reporting completeness and transparency of systematic reviews of prognostic prediction models for COVID-19 was poor: a methodological overview of systematic reviews. Journal of Clinical Epidemiology 2024;167:111264. doi: 10.1016/j.jclinepi.2024.111264.\n\n\n\n\n\n\n\nBougioukas KI, Karakasis P, Pamporis K, Bouras E, Haidich A-B. amstar2Vis: an R package for presenting the critical appraisal of systematic reviews based on the items of AMSTAR 2. Research Synthesis Methods 2024. doi: 10.1002/jrsm.1705.\n\n\n\n\n\n\n\nPuchner KP, Giannakou V, Veizis A, Bougioukas K, Hargreaves S, Benos A, Kondilis E. COVID-19 vaccination roll-out and uptake among refugees and migrants in Greece: a retrospective analysis of national vaccination routine data. Public Health 2024; 229:84–87. doi: 10.1016/j.puhe.2024.01.010.\n\n\n\n\n\n\n\nSeliniotaki AK, Bougioukas KI, Lithoxopoulou M, Moutzouri S, Diamanti E, Ziakas N, Mataftsi A. Mydriasis for retinopathy of prematurity screening in Europe: A cross-sectional online survey. European Journal of Ophthalmology 2024; 0:0–0. doi: 10.1177/11206721241234952.\n\n\n\n\n\n\n\nSafouris A, Palaiodimou L, Katsanos AH, Kargiotis O, Bougioukas KI, Psychogios K, Sidiropoulou T, Spiliopoulos S, Psychogios M-N, Magoufis G, Turc G, Tsivgoulis G. Overview of systematic reviews comparing endovascular to best medical treatment for large-vessel occlusion acute ischaemic stroke: an umbrella review. Therapeutic Advances in Neurological Disorders 2004. doi: 10.1177/17562864241246938.\n\n\n\n\n\n\n\nPamporis K, Karakasis P, Sagris M, Zarifis I, Bougioukas KI, Pagkalidou E, Milaras N, Samaras A, Theofilis P, Fragakis N, Tousoulis D, Xanthos T, Giannakoulas G. Mineralocorticoid receptor antagonists in heart failure with reduced ejection fraction: a systematic review and network meta-analysis of 32 randomized trials. Current Problems in Cardiology 2024; 0:0–0. doi: 10.1016/j.cpcardiol.2024.102615.\n\n\n\n\n\n\n\nLunny C, Kanji S, Thabet P, Haidich A-B, Bougioukas KI, Pieper D. Assessing the methodological quality and risk of bias of systematic reviews: primer for authors of overviews of systematic reviews. BMJ Medicine 2024; 3. doi: 10.1136/bmjmed-2023-000604.\n\n\n\n\n\n\n\nSeliniotaki AK, Lithoxopoulou M, Virgiliou C, Gika H, Dokoumetzidis A, Bougioukas KI, et al. Efficacy and Safety of Mydriatic Microdrops for Retinopathy of Prematurity Screening: The MyMiROPS Randomized Clinical Trial. JAMA Ophthalmology 2024. 10.1001/jamaophthalmol.2024.5462.\n\n\n\n\n\n\n\nBartzoulianou RC, Coleman AL, Wilson MR, Harris A, Bougioukas KI, Pappas T, et al. Factors Associated with Corneal Hysteresis in an Elderly White Population: The Thessaloniki Eye Study. Journal of Glaucoma 2025; doi: 10.1097/IJG.0000000000002541.\n\n\n\n\n\n\n\nKarakosta C, Samiotaki M, Bisoukis A, Bougioukas KI, Panayotou G, Papaconstantinou D, et al. Differential Signaling Pathways Identified in Aqueous Humor, Anterior Capsule, and Crystalline Lens of Age-Related, Diabetic, and Post-Vitrectomy Cataract. Proteomes 2025;13:7. 10.3390/proteomes13010007.\n\n\n\n\n\n\n\nAnestiadou E, Stamiris S, Ioannidis O, Symeonidis S, Bitsianis S, Bougioukas K, Karagiannis T, Kotidis E, Pramateftakis M-G, Mantzoros I, et al. Comparison of Negative Pressure Wound Therapy Systems and Conventional Non-Pressure Dressings on Surgical Site Infection Rate After Stoma Reversal: Systematic Review and Meta-Analysis of Randomized Controlled Trials. Journal of Clinical Medicine 2025; 14(5):1654. 10.3390/jcm14051654.\n\n\n\n\n\n\n\nYing X, Bougioukas KI, Pieper D, Mayo-Wilson E. Weighted corrected covered area (wCCA): A measure of informational overlap among reviews. Research Synthesis Methods 2025; 16(4):701-708. 10.1017/rsm.2025.19.\n\n\n\n\n\n\n\nKarvouniaris M, Koulenti D, Bougioukas KI, Pagkalidou E, Paramythiotou E, Haidich A-B. Nebulized Antibiotics for Preventing and Treating Gram-Negative Respiratory Infections in Critically Ill Patients: An Overview of Reviews. Antibiotics 2025; 14(4):370. 10.3390/antibiotics14040370.\n\n\n\n\n\n\n\nKarakosta C, Kiraly P, Bisoukis A, Bougioukas KI, Fischer D.M. Morphological and Functional Outcomes in the Long-Term Natural Course of Peripapillary Pachychoroid Syndrome. Ophthalmology and Therapy  2025; 14:2785–2799. 10.1007/s40123-025-01226-8.\n\n\n\n\n\n\n\nKarakosta C, Samiotaki M, Bisoukis A, Bougioukas KI, Panayotou G, Kyriakidou N, Moschou K, Moschos M.M. Comparative Proteomic Analysis of Aqueous Humor, Anterior Lens Capsules, and Crystalline Lenses in Different Human Cataract Subtypes Versus Healthy Controls. Proteomes 2025; 13(4):62. 10.3390/proteomes13040062."
  },
  {
    "objectID": "publications.html#books-and-chapters",
    "href": "publications.html#books-and-chapters",
    "title": "Publications and metrics",
    "section": "Books and Chapters",
    "text": "Books and Chapters\n Bougioukas K. Practical Statistics in Medicine with R (in progress; Chapman & Hall/CRC Press | Taylor & Francis Group) 2024.\n \n \n Bougioukas K, Haidich A-B. Medical Biostatistics: Basic Concepts. In: Papademetriou V, Andreadis EA, Geladari C, editors. Manag. Hypertens. Curr. Pract. Appl. Landmark Trials, Cham: Springer International Publishing; 2019, p. 19–53. doi: 10.1007/978-3-319-92946-0_2."
  },
  {
    "objectID": "publications.html#international-conference-proceedings",
    "href": "publications.html#international-conference-proceedings",
    "title": "Publications and metrics",
    "section": "International Conference Proceedings",
    "text": "International Conference Proceedings\nBougioukas KI, Diakonidis T, Mavromanoli A, Haidich AB. ccaR: a package for assessing primary study overlap across SRs in overviews. ESMARConf2023, 27-31 March 2023. [YouTube]\nBougioukas K, Vounzoulaki E, Mantsiou M, Savvides E, Karakosta C, Diakonidis T, Tsapas A, Haidich AB. Methods for depicting overlap in overviews of systematic reviews: The ccaR package. AUEB-NKUA-Indiana-FSFH Conference in Biostatistics & Health Analytics, Aegina, 4-6 July 2022.\nBougioukas KI, Pagkalidou E, Avgerinou E, Tsapas A, Ntzani E, Smyrnakis E, Haidich AB. Overviews of Systematic Reviews with drugs, herbal medicine or dietary supplements. 9th Conference of the Eastern Mediterranean Region of the International Biometrics Society (EMR‐IBS), Thessaloniki, Greece, 8-12 May 2017."
  }
]